{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classical_methods_experiments.ipynb","provenance":[{"file_id":"1Stwx2C0_YLkjie0n_GlK_-jxAqLlcds1","timestamp":1615244133879},{"file_id":"1MZJu8rFTDTJZI74mGhTgldlStguVjUps","timestamp":1614173101147},{"file_id":"1K2wdOGisKKa3l4ZDebkmm-XkWwmLncQB","timestamp":1613576489687}],"collapsed_sections":[],"authorship_tag":"ABX9TyOH0o0S1CoXEkG8N2nqa9Hv"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HuuWP8tqdwuo"},"source":["# üß´ üß™ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º–∏ *unsupervised* –º–µ—Ç–æ–¥–∞–º–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π"]},{"cell_type":"markdown","metadata":{"id":"EPIzrTvamj1z"},"source":["### üåê –£—Å—Ç–∞–Ω–æ–≤–∫–∞ [pyod](https://github.com/yzhao062/pyod)"]},{"cell_type":"code","metadata":{"id":"0LtDUqs-mxbQ"},"source":["# !pip3 install pyod"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7zAKhE3pOLka"},"source":["## üíÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZ3bvuYgOJ_b","executionInfo":{"status":"ok","timestamp":1615382714607,"user_tz":-180,"elapsed":1982,"user":{"displayName":"Sait Sharipov","photoUrl":"","userId":"06615756036365395502"}},"outputId":"f70e1abc-7889-4559-96bd-c8021d60124d"},"source":["from bs4 import BeautifulSoup\n","from gensim.parsing.preprocessing import remove_stopwords\n","from gensim.parsing.preprocessing import strip_short\n","from gensim.parsing.preprocessing import strip_non_alphanum\n","from gensim.parsing.preprocessing import strip_numeric\n","from gensim.utils import tokenize\n","import nltk; nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","\n","\n","def strip_html_tags(text):\n","    \"\"\"–£–¥–∞–ª–µ–Ω–∏–µ html tags –∏–∑ —Ç–µ–∫—Å—Ç–∞.\"\"\"\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    stripped_text = soup.get_text(separator=\" \")\n","    return stripped_text\n","\n","\n","def get_wordnet_pos(word):\n","    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","    return tag_dict.get(tag, wordnet.NOUN)\n","\n","\n","def preprocess_text(text):\n","    text = strip_html_tags(text)  # —É–¥–∞–ª–µ–Ω–∏–µ html tags\n","    text = strip_non_alphanum(text) # –∑–∞–º–µ–Ω–∏–ª–∏ –≤—Å–µ –Ω–µ–±—É–∫–≤–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –ø—Ä–æ–±–µ–ª\n","    text = strip_numeric(text) # —É–¥–∞–ª–∏–ª–∏ –≤—Å–µ —Ü–∏—Ñ—Ä—ã\n","    text = remove_stopwords(text) # —É–¥–∞–ª–∏–ª–∏ –≤—Å–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n","    # text = strip_short(text, minsize=2) # —É–¥–∞–ª–∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞\n","    word_list = list(tokenize(text, deacc=True, to_lower=True)) # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, deacc - –∏–∑–±–∞–≤–ª—è–µ—Ç –æ—Ç —É–¥–∞—Ä–µ–Ω–∏–π\n","    word_list = [WordNetLemmatizer().lemmatize(word) for word in word_list] # –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n","    return ' '.join(word for word in word_list)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"72kLNO3zHEqB"},"source":["## üß¨ –≠–∫–ø–µ—Ä–∏–º–µ–Ω—Ç—ã"]},{"cell_type":"markdown","metadata":{"id":"0-trVVbaHCxV"},"source":["### Isolation Forest, LOF, kNN, COPOD"]},{"cell_type":"code","metadata":{"id":"Nzkj-zxjGPrK"},"source":["# import numpy as np\n","# import pandas as pd\n","\n","# from sklearn.datasets import fetch_20newsgroups\n","# from sklearn.utils import shuffle\n","# from sklearn.feature_extraction.text import TfidfVectorizer\n","# from sklearn.metrics import roc_auc_score\n","\n","# import pyod\n","# from pyod.models import iforest\n","# from pyod.models import lof\n","# from pyod.models import knn\n","# from pyod.models import copod\n","\n","# c = 0.1  # –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º\n","\n","# categories = [\"comp.graphics\",\n","#               'talk.politics.mideast',\n","#               \"rec.sport.hockey\",\n","#               \"sci.med\",\n","#               \"sci.space\",\n","#               'misc.forsale',\n","#               'soc.religion.christian',\n","#               'talk.politics.misc']\n","\n","# experimant_cnt = 0\n","# all_experiments = len(categories) * (len(categories) -  1)\n","# auc_list_iforest = []\n","# auc_list_lof = []\n","# auc_list_knn = []\n","# auc_list_copod = []\n","\n","# # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏\n","# dataset = {}\n","# for cat in categories:\n","#     # –ó–∞–≥—Ä—É–∑–∫–∞\n","#     dataset[cat] = fetch_20newsgroups(subset='all', categories=[cat],\n","#                             shuffle=True, random_state=123,\n","#                             remove=('headers', 'footers'), return_X_y=True)[0]\n","#     # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞\n","#     dataset[cat] = [preprocess_text(text) for text in dataset[cat]]\n","\n","# # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –ø–∞—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n","# for i in range(len(categories)):\n","#     for j in range(len(categories)):\n","#         if i == j:\n","#             continue\n","\n","#         c1 = categories[i]\n","#         c2 = categories[j]\n","\n","#         experimant_cnt += 1\n","\n","#         # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –∏ –∞–Ω–æ–º–∞–ª—å–Ω–æ–π –≤—ã–±–æ—Ä–æ–∫\n","#         normal_data = dataset[c1]\n","#         anomal_data = dataset[c2][:min(int(c * len(normal_data)) + 1, len(dataset[c2]))]\n","#         all_data = normal_data + anomal_data\n","\n","#         # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\n","#         vectorizer = TfidfVectorizer()\n","#         all_data_tf = vectorizer.fit_transform(all_data).toarray()\n","\n","#         # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–æ–∫\n","#         x = all_data_tf\n","#         y = np.array([False] * len(normal_data) + [True] * len(anomal_data))\n","#         all_data, x, y = shuffle(all_data, x, y, random_state=123)\n","\n","#         # –ó–∞–¥–∞–µ–º –º–æ–¥–µ–ª–∏\n","\n","#         # Isolation Forest\n","#         iforest_clf = iforest.IForest(\n","#             contamination=0.1,\n","#             n_estimators=5000,\n","#             max_samples=1.0,\n","#             bootstrap=True,\n","#             random_state=123,\n","#             n_jobs=-1,\n","#         )\n","\n","#         # LOF\n","#         lof_clf = lof.LOF(\n","#             contamination=0.1,\n","#             n_neighbors=5,\n","#             metric='canberra',\n","#             n_jobs=-1,\n","#         )\n","\n","#         #kNN\n","#         knn_clf = knn.KNN(\n","#             contamination=0.1,\n","#             n_neighbors=3,\n","#             method='largest',\n","#             metric='canberra',\n","#             n_jobs=-1,\n","#         )\n","\n","#         # COPOD\n","#         copod_clf = copod.COPOD(contamination=0.1)\n","\n","#         # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫—É ROC AUC\n","#         auc_iforest = iforest_clf.fit_predict_score(x, y)\n","#         auc_lof = lof_clf.fit_predict_score(x, y)\n","#         auc_knn = knn_clf.fit_predict_score(x, y)\n","#         auc_copod = copod_clf.fit_predict_score(x, y)\n","\n","#         # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–∫–∏\n","#         auc_list_iforest.append(auc_iforest)\n","#         auc_list_lof.append(auc_lof)\n","#         auc_list_knn.append(auc_knn)\n","#         auc_list_copod.append(auc_copod)\n","\n","#         # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n","#         print(\"-\" * 50)\n","#         print(\"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç ‚Ññ{}/{}  —Å normal = {}, anomal = {}\".format(\n","#             experimant_cnt, all_experiments, c1, c2))\n","#         print(\"auc_iforest = \", auc_iforest)\n","#         print(\"auc_lof = \", auc_lof)\n","#         print(\"auc_knn = \", auc_knn)\n","#         print(\"auc_copod = \", auc_copod)\n","#         print(\"-\" * 50)\n","\n","\n","# auc_np = np.array(auc_list_iforest)\n","# print(\"*\" * 50)\n","# print(\"IFOREST –ú–µ–¥–∏–∞–Ω–∞ auc = {}\".format(np.median(auc_np)))\n","# print(\"IFOREST –°—Ä–µ–¥–Ω–µ–µ auc = {}\".format(np.mean(auc_np)))\n","# print(\"*\" * 50)\n","\n","# auc_np = np.array(auc_list_lof)\n","# print(\"*\" * 50)\n","# print(\"LOF –ú–µ–¥–∏–∞–Ω–∞ auc = {}\".format(np.median(auc_np)))\n","# print(\"LOF –°—Ä–µ–¥–Ω–µ–µ auc = {}\".format(np.mean(auc_np)))\n","# print(\"*\" * 50)\n","\n","# auc_np = np.array(auc_list_knn)\n","# print(\"*\" * 50)\n","# print(\"kNN –ú–µ–¥–∏–∞–Ω–∞ auc = {}\".format(np.median(auc_np)))\n","# print(\"kNN –°—Ä–µ–¥–Ω–µ–µ auc = {}\".format(np.mean(auc_np)))\n","# print(\"*\" * 50)\n","\n","# auc_np = np.array(auc_list_copod)\n","# print(\"*\" * 50)\n","# print(\"COPOD –ú–µ–¥–∏–∞–Ω–∞ auc = {}\".format(np.median(auc_np)))\n","# print(\"COPOD –°—Ä–µ–¥–Ω–µ–µ auc = {}\".format(np.mean(auc_np)))\n","# print(\"*\" * 50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ga-RZT8QHJIT"},"source":["### AE, VAE"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":736},"id":"o8lRveSUHLF1","executionInfo":{"status":"error","timestamp":1615383524767,"user_tz":-180,"elapsed":812117,"user":{"displayName":"Sait Sharipov","photoUrl":"","userId":"06615756036365395502"}},"outputId":"491afc91-98b6-42c3-ce82-0153c7ff9a8a"},"source":["import numpy as np\r\n","import pandas as pd\r\n","\r\n","from sklearn.datasets import fetch_20newsgroups\r\n","from sklearn.utils import shuffle\r\n","from sklearn.feature_extraction.text import TfidfVectorizer\r\n","from sklearn.metrics import roc_auc_score\r\n","\r\n","import pyod\r\n","from pyod.models import auto_encoder\r\n","from pyod.models import vae\r\n","\r\n","c = 0.1  # –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º\r\n","\r\n","categories = [\"comp.graphics\",\r\n","              'talk.politics.mideast',\r\n","              \"rec.sport.hockey\",\r\n","              \"sci.med\",\r\n","              \"sci.space\",\r\n","              'misc.forsale',\r\n","              'soc.religion.christian',\r\n","              'talk.politics.misc']\r\n","\r\n","experimant_cnt = 0\r\n","all_experiments = len(categories) * (len(categories) -  1)\r\n","auc_list_ae = []\r\n","auc_list_vae = []\r\n","\r\n","# –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏\r\n","dataset = {}\r\n","for cat in categories:\r\n","    # –ó–∞–≥—Ä—É–∑–∫–∞\r\n","    dataset[cat] = fetch_20newsgroups(subset='all', categories=[cat],\r\n","                            shuffle=True, random_state=123,\r\n","                            remove=('headers', 'footers'), return_X_y=True)[0]\r\n","    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞\r\n","    dataset[cat] = [preprocess_text(text) for text in dataset[cat]]\r\n","\r\n","# –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –ø–∞—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π\r\n","for i in range(len(categories)):\r\n","    for j in range(len(categories)):\r\n","        if i == j:\r\n","            continue\r\n","\r\n","        c1 = categories[i]\r\n","        c2 = categories[j]\r\n","\r\n","        experimant_cnt += 1\r\n","\r\n","        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –∏ –∞–Ω–æ–º–∞–ª—å–Ω–æ–π –≤—ã–±–æ—Ä–æ–∫\r\n","        normal_data = dataset[c1]\r\n","        anomal_data = dataset[c2][:min(int(c * len(normal_data)) + 1, len(dataset[c2]))]\r\n","        all_data = normal_data + anomal_data\r\n","\r\n","        # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è\r\n","        vectorizer = TfidfVectorizer()\r\n","        all_data_tf = vectorizer.fit_transform(all_data).toarray()\r\n","\r\n","        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–æ–∫\r\n","        x = all_data_tf\r\n","        y = np.array([False] * len(normal_data) + [True] * len(anomal_data))\r\n","        all_data, x, y = shuffle(all_data, x, y, random_state=123)\r\n","\r\n","        # –ó–∞–¥–∞–µ–º –º–æ–¥–µ–ª–∏\r\n","\r\n","        # AE\r\n","        ae_clf = auto_encoder.AutoEncoder(\r\n","            hidden_neurons=[256, 128, 64, 64, 128, 256],\r\n","            hidden_activation='relu',\r\n","            output_activation='sigmoid',\r\n","            optimizer='adam',\r\n","            epochs=35,\r\n","            batch_size=16,\r\n","            dropout_rate=0.3,\r\n","            l2_regularizer=0.1,\r\n","            validation_size=0.1,\r\n","            preprocessing=True,\r\n","            verbose=0,\r\n","            random_state=123,\r\n","            contamination=0.1\r\n","        )\r\n","\r\n","        # VAE\r\n","        # vae_clf = vae.VAE(\r\n","        #     contamination=0.1,\r\n","        #     encoder_neurons=[256, 128, 64],\r\n","        #     decoder_neurons=[64, 128, 256],\r\n","        #     latent_dim=5,\r\n","        #     hidden_activation='relu',\r\n","        #     output_activation='sigmoid',\r\n","        #     optimizer='adam',\r\n","        #     epochs=16,\r\n","        #     batch_size=64,\r\n","        #     dropout_rate=0.3,\r\n","        #     l2_regularizer=0.1,\r\n","        #     validation_size=0.1,\r\n","        #     preprocessing=True,\r\n","        #     verbose=0,\r\n","        #     random_state=123,\r\n","        # )\r\n","\r\n","        # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫—É ROC AUC\r\n","\r\n","        ae_clf.fit(x)\r\n","        y_predict_ae = ae_clf.decision_function(x)\r\n","        auc_ae = roc_auc_score(y, y_predict_ae)\r\n","\r\n","        # vae_clf.fit(x)\r\n","        # y_predict_vae = vae_clf.decision_function(x)\r\n","        # auc_vae = roc_auc_score(y, y_predict_vae)\r\n","\r\n","        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–∫–∏\r\n","        auc_list_ae.append(auc_ae)\r\n","        # auc_list_vae.append(auc_vae)\r\n","\r\n","        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\r\n","        print(\"-\" * 50)\r\n","        print(\"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç ‚Ññ{}/{}  —Å normal = {}, anomal = {}\".format(\r\n","            experimant_cnt, all_experiments, c1, c2))\r\n","        print(\"auc_ae = \", auc_ae)\r\n","        # print(\"auc_vae = \", auc_vae)\r\n","        print(\"-\" * 50)\r\n","\r\n","\r\n","auc_np = np.array(auc_list_ae)\r\n","print(\"*\" * 50)\r\n","print(\"AE –ú–µ–¥–∏–∞–Ω–∞ auc = {}\".format(np.median(auc_np)))\r\n","print(\"AE –°—Ä–µ–¥–Ω–µ–µ auc = {}\".format(np.mean(auc_np)))\r\n","print(\"*\" * 50)\r\n","\r\n","# auc_np = np.array(auc_list_vae)\r\n","# print(\"*\" * 50)\r\n","# print(\"VAE –ú–µ–¥–∏–∞–Ω–∞ auc = {}\".format(np.median(auc_np)))\r\n","# print(\"VAE –°—Ä–µ–¥–Ω–µ–µ auc = {}\".format(np.mean(auc_np)))\r\n","# print(\"*\" * 50)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--------------------------------------------------\n","–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç ‚Ññ1/56  —Å normal = comp.graphics, anomal = talk.politics.mideast\n","auc_ae =  0.8618621138074962\n","--------------------------------------------------\n","--------------------------------------------------\n","–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç ‚Ññ2/56  —Å normal = comp.graphics, anomal = rec.sport.hockey\n","auc_ae =  0.8150995238794387\n","--------------------------------------------------\n","--------------------------------------------------\n","–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç ‚Ññ3/56  —Å normal = comp.graphics, anomal = sci.med\n","auc_ae =  0.8223304738133692\n","--------------------------------------------------\n","--------------------------------------------------\n","–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç ‚Ññ4/56  —Å normal = comp.graphics, anomal = sci.space\n","auc_ae =  0.8206577595066804\n","--------------------------------------------------\n"],"name":"stdout"},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-ee2c9d63b09f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫—É ROC AUC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mae_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0my_predict_ae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mauc_ae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict_ae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyod/models/auto_encoder.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    249\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                                         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                                         verbose=self.verbose).history\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0;31m# Reverse the operation for consistency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_neurons_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[13127,13127] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/sequential_4/dense_37/MatMul_1 (defined at /usr/local/lib/python3.7/dist-packages/pyod/models/auto_encoder.py:251) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_23669]\n\nFunction call stack:\ntrain_function\n"]}]},{"cell_type":"code","metadata":{"id":"rqL21ik8T9ap","executionInfo":{"status":"ok","timestamp":1615404293754,"user_tz":-180,"elapsed":1764,"user":{"displayName":"Sait Sharipov","photoUrl":"","userId":"06615756036365395502"}},"outputId":"8997e2b9-3965-4827-aa36-9850f3ca0d9d","colab":{"base_uri":"https://localhost:8080/"}},"source":["import multiprocessing\n","multiprocessing.cpu_count()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"pgDccHEhmTbq"},"source":[""],"execution_count":null,"outputs":[]}]}