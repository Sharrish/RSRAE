{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RSRAE_for_20newsgroups.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRalzZJkmP_b"
      },
      "source": [
        "## Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt_FOROwmCiT",
        "outputId": "a271bf79-ef0f-4828-9ca6-e3ae04539270"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "\n",
        "c = 0.15  # отношение количества аномальных экземпляров к нормальным\n",
        "\n",
        "# загужаем данные\n",
        "normal_data = fetch_20newsgroups(subset='all', categories=['sci.electronics'],\n",
        "                               shuffle=True, random_state=123, \n",
        "                               remove=['headers', 'footers'], return_X_y=True)[0]\n",
        "anomal_data = fetch_20newsgroups(subset='all', categories=['talk.politics.mideast'],\n",
        "                               shuffle=True, random_state=123,\n",
        "                               remove=['headers', 'footers'],\n",
        "                               return_X_y=True)[0][:int(c * len(normal_data)) + 1]\n",
        "\n",
        "# # приводим к одинаковой длине\n",
        "# min_len = max(len(normal_data), len(anomal_data))\n",
        "# normal_data = normal_data[:min_len]\n",
        "# anomal_data = anomal_data[:min_len]\n",
        "print(\"Количество нормальных экземпляров = {}\".format(len(normal_data)))\n",
        "print(\"Количество аномальных экземпляров = {}\".format(len(anomal_data)))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Количество нормальных экземпляров = 984\n",
            "Количество аномальных экземпляров = 148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BYPvWKZmWh8"
      },
      "source": [
        "## Предобработка текстовых данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZIjvvyxmNoV",
        "outputId": "44bd59c4-93e3-4610-a7b9-0cca3a9b13a7"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.parsing.preprocessing import strip_short\n",
        "from gensim.parsing.preprocessing import strip_non_alphanum\n",
        "from gensim.parsing.preprocessing import strip_numeric\n",
        "from gensim.utils import tokenize\n",
        "import nltk; nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    \"\"\"Удаление html tags из текста.\"\"\"\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text(separator=\" \")\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = strip_html_tags(text)  # удаление html tags\n",
        "    text = strip_non_alphanum(text) # заменили все небуквенные символы на пробел\n",
        "    text = strip_numeric(text) # удалили все цифры\n",
        "    text = remove_stopwords(text) # удалили все стоп-слова\n",
        "    text = strip_short(text, minsize=2) # удалили короткие слова\n",
        "    word_list = list(tokenize(text, deacc=True, to_lower=True)) # токенизация, deacc - избавляет от ударений\n",
        "    word_list = [WordNetLemmatizer().lemmatize(word) for word in word_list] # лемматизация\n",
        "    return ' '.join(word for word in word_list)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DjSyanxmY5t"
      },
      "source": [
        "normal_data = [preprocess_text(text) for text in normal_data]\n",
        "anomal_data = [preprocess_text(text) for text in anomal_data]\n",
        "all_data = normal_data + anomal_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26fBhnEamb0R",
        "outputId": "0553b1aa-1a4a-4e21-c9cc-e17252bd9eaa"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "len_all_data = np.array([len(text.split(' ')) for text in all_data])\n",
        "# print(len(len_all_data[len_all_data <= 150]))\n",
        "print(\"mean length of sentence: \" + str(len_all_data.mean()))\n",
        "print(\"max length of sentence: \" + str(len_all_data.max()))\n",
        "print(\"std dev length of sentence: \" + str(len_all_data.std()))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean length of sentence: 110.97968197879858\n",
            "max length of sentence: 6252\n",
            "std dev length of sentence: 270.34016089125856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMPrZMJ8nI67"
      },
      "source": [
        "## Tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enIVjuX8mdjY",
        "outputId": "e54a6bdd-b369-41e8-9867-9093301d8ad2"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(all_data)\n",
        "\n",
        "normal_sequences = vectorizer.transform(normal_data).toarray()\n",
        "anomal_sequences = vectorizer.transform(anomal_data).toarray()\n",
        "print(normal_sequences.shape)\n",
        "print(anomal_sequences.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(984, 14685)\n",
            "(148, 14685)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ccyz5mD3oMbx"
      },
      "source": [
        "## Формирование выборок"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLW5cvnenwY1",
        "outputId": "d72e18ac-ba77-42e8-a3d9-b07c0f6af9b9"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "x = np.concatenate((normal_sequences, anomal_sequences))\n",
        "y = np.array([False] * normal_sequences.shape[0] + [True] * anomal_sequences.shape[0])\n",
        "\n",
        "all_data, x, y = shuffle(all_data, x, y, random_state=123)\n",
        "print(\"Всего экземпляров = {}\".format(len(all_data)))\n",
        "print(\"(Кол-во текстов, число признаков текста) = {}\".format(x.shape))\n",
        "print(\"Кол-во меток = {}\".format(len(y)))\n",
        "print(\"Кол-во нормальных экземпляров = {}\".format(normal_sequences.shape[0]))\n",
        "print(\"Кол-во аномальных экземпляров = {}\".format(anomal_sequences.shape[0]))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Всего экземпляров = 1132\n",
            "(Кол-во текстов, число признаков текста) = (1132, 14685)\n",
            "Кол-во меток = 1132\n",
            "Кол-во нормальных экземпляров = 984\n",
            "Кол-во аномальных экземпляров = 148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYXETryTvNmo"
      },
      "source": [
        "## Функция Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w__X67CvvKOk"
      },
      "source": [
        "def cosine_similarity(x_predict, x):\n",
        "    if type(x_predict) is np.ndarray:\n",
        "        flat_output = np.reshape(x_predict, (np.shape(x)[0], -1))\n",
        "        flat_input = np.reshape(x_predict, (np.shape(x)[0], -1))\n",
        "        sum = np.sum(flat_output * flat_input, -1)\n",
        "        norm1 = np.linalg.norm(flat_output, axis=-1) + 0.000001\n",
        "        norm2 = np.linalg.norm(flat_input, axis=-1) + 0.000001 \n",
        "        return -(sum / norm1 / norm2)\n",
        "    else:\n",
        "        # ДЛЯ НЕ ПОЛНОСВЯЗНЫХ СЛОЕВ НУЖЕН ДРУГОЙ shape\n",
        "        flat_output = tf.reshape(tensor=x_predict, shape=[x.shape.as_list()[0], -1])\n",
        "        flat_input = tf.reshape(tensor=x_predict, shape=[x.shape.as_list()[0], -1])\n",
        "        sum = tf.math.reduce_sum(tf.math.multiply(flat_output, flat_input), axis=-1)\n",
        "        norm1 = tf.norm(flat_output, axis=-1) + 0.000001\n",
        "        norm2 = tf.norm(flat_input, axis=-1) + 0.000001\n",
        "        return -(tf.math.divide(tf.math.divide(sum, norm1), norm2))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWA3wKyKu0Rw"
      },
      "source": [
        "## RSRAE model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k5W1W_roXvH",
        "outputId": "9556a612-19b3-45d3-f2f2-3886b7dd2a21"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version = {}\".format(tf.__version__)) # текущая версия tf\n",
        "\n",
        "from tensorflow.keras import Model, optimizers, metrics\n",
        "from tensorflow.keras.layers import Layer, Flatten, Dense, BatchNormalization\n",
        "\n",
        "# from tensorflow.keras import activations, Sequential, Input\n",
        "# from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Reshape\n",
        "# from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# Задаем random_seed для tensorflow и numpy\n",
        "random_seed = 123\n",
        "tf.random.set_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "# Sets the default float type\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "# Set random seed\n",
        "tf.random.set_seed(123)\n",
        "np.random.seed(123)\n",
        "\n",
        "\n",
        "class RSR(Layer):\n",
        "    \"\"\"\n",
        "    Robust Subspace Recovery (RSR) layer.\n",
        "    Робастный слой, восстанавливающий подпространство. Задача данного слоя - отобразить\n",
        "    закодированные энкодером данные в подпростраство так, чтобы после их обратного\n",
        "    отображения декодером дивергенция между экземпляром исходных данных и его образом,\n",
        "    полученным от автоэнкодера была незначительной для нормального экземпляра и была\n",
        "    большой для аномального экземпляра. \n",
        "\n",
        "    # Example\n",
        "    ```\n",
        "        z_rsr, A = RSR(intrinsic_size=10)(z)\n",
        "    ```\n",
        "    # Arguments\n",
        "        intrinsic_size: размерность z_rsr.\n",
        "    # Input shape\n",
        "        2D tensor with shape: `(n_samples, n_features)` after encoding.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(n_samples, intrinsic_size)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, intrinsic_size: int, name=\"RSR_layer\", **kwargs):\n",
        "        super(RSR, self).__init__(name=name, **kwargs)\n",
        "        # Если присваивать экземпляр слоя, как атрибут другого слоя, то хорошей\n",
        "        # практикой делать создавать такие подслои в __init__ (поскольку подслои обычно\n",
        "        # имеют метод build, они будут собраны, когда будет собран внешний слой). \n",
        "        self.flatten = Flatten()\n",
        "        self.intrinsic_size = intrinsic_size\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Определяет веса слоя, а именно задает матрицу A.\"\"\"\n",
        "        self.A = self.add_weight(name=\"A\",\n",
        "                                 shape=[int(input_shape[-1]), self.intrinsic_size],\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True,)\n",
        "        \n",
        "    def call(self, z):\n",
        "        \"\"\"\n",
        "        Логика слоя. Умножение выхода энкодера - вектора z на матрицу A.\n",
        "        Возвращает отображенный z_rsr и матрицу A, которая потребуется далее.\n",
        "        \"\"\"\n",
        "        z = self.flatten(z)\n",
        "        z_rsr = tf.linalg.matmul(z, self.A) \n",
        "        return z_rsr\n",
        "\n",
        "    # Опционально, пользовательский слой может быть сериализован реализацией метода \n",
        "    # get_config и метода класса (@classmethod) from_config.\n",
        "    def get_config(self):\n",
        "        config = super(Layer, self).get_config()\n",
        "        config.update({'intrinsic_size': self.intrinsic_size})\n",
        "        return config\n",
        "\n",
        "    # На самом деле нет необходимости определять `from_config` здесь, поскольку \n",
        "    # возвращение `cls(**config)` - поведение по умолчанию.\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "class L2Normalization(Layer):\n",
        "    \"\"\"Слой для l_2 нормализации, который будет применяться к выходу RSR layer.\"\"\"\n",
        "    \n",
        "    def __init__(self, name=\"L2Normalization\", **kwargs):\n",
        "        super(L2Normalization, self).__init__(name=name, **kwargs)\n",
        "\n",
        "    def call(self, z_rsr):\n",
        "        \"\"\"\n",
        "        Выполняет l_2 нормализацию векторов, полученных после применения RSR layer\n",
        "        вдоль оси, соответсвующей числу признаков. То есть производится нормализация\n",
        "        каждого экземпляра выборки, в результате которой признаки экземпляров будут\n",
        "        находиться в отрезке [-1; 1].\n",
        "        \"\"\"\n",
        "        z_tilde = tf.math.l2_normalize(z_rsr, axis=-1)\n",
        "        return z_tilde\n",
        "\n",
        "    # Опционально, пользовательский слой может быть сериализован реализацией метода \n",
        "    # get_config и метода класса (@classmethod) from_config.\n",
        "    def get_config(self):\n",
        "        config = super(Layer, self).get_config()\n",
        "        return config\n",
        "\n",
        "    # На самом деле нет необходимости определять `from_config` здесь, поскольку \n",
        "    # возвращение `cls(**config)` - поведение по умолчанию.\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "class Encoder(Layer):\n",
        "    \"\"\"\n",
        "    Класс для encoder модели RSRAE. Отображает исходные данные input_data в вектор z,\n",
        "    кодирующий исходные данные.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 hidden_layer_dimensions,\n",
        "                 activation,\n",
        "                 flag_bn=True, \n",
        "                 name=\"Encoder\",\n",
        "                 **kwargs):\n",
        "        super(Encoder, self).__init__(name=name, **kwargs)\n",
        "        self.hidden_layer_dimensions = hidden_layer_dimensions\n",
        "        self.activation = activation\n",
        "        self.flag_bn = flag_bn\n",
        "        self.dense0 = Dense(hidden_layer_dimensions[0], activation=activation,\n",
        "                            name='encoder_0')\n",
        "        self.dense1 = Dense(hidden_layer_dimensions[1], activation=activation,\n",
        "                            name='encoder_1')\n",
        "        self.dense2 = Dense(hidden_layer_dimensions[2], activation=activation,\n",
        "                            name='encoder_2')\n",
        "        if flag_bn:\n",
        "            self.batch_normalization0 = BatchNormalization(name=\"encoder_bn_layer_0\")\n",
        "            self.batch_normalization1 = BatchNormalization(name=\"encoder_bn_layer_1\")\n",
        "            self.batch_normalization2 = BatchNormalization(name=\"encoder_bn_layer_2\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Отображние исходных данных x -> в закодированный вектор z.\"\"\"\n",
        "        x = inputs\n",
        "        x = self.dense0(x)\n",
        "        if self.flag_bn:\n",
        "            x = self.batch_normalization0(x)\n",
        "        x = self.dense1(x)\n",
        "        if self.flag_bn:\n",
        "            x = self.batch_normalization1(x)\n",
        "        x = self.dense2(x)\n",
        "        if self.flag_bn:\n",
        "            x = self.batch_normalization2(x)\n",
        "        z = x\n",
        "        return z\n",
        "    \n",
        "    # Опционально, пользовательский слой может быть сериализован реализацией метода \n",
        "    # get_config и метода класса (@classmethod) from_config.\n",
        "    def get_config(self):\n",
        "        config = super(Layer, self).get_config()\n",
        "        config.update({'hidden_layer_dimensions': self.hidden_layer_dimensions})\n",
        "        config.update({'activation': self.activation})\n",
        "        config.update({'flag_bn': self.flag_bn})\n",
        "        return config\n",
        "\n",
        "    # На самом деле нет необходимости определять `from_config` здесь, поскольку \n",
        "    # возвращение `cls(**config)` - поведение по умолчанию.\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "class Decoder(Layer):\n",
        "    \"\"\"\n",
        "    Класс для decoder модели RSRAE. Отображает вектор z_rsr, полученный в результате\n",
        "    кодирования исходных данных в вектор z, и последующим отображением вектора z при\n",
        "    помощи RSR layer (x -> z -> z_rsr), обратно в пространство исходных данных \n",
        "    (z_rsr -> x_tilde).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 inputs_dim,\n",
        "                 hidden_layer_dimensions,\n",
        "                 activation,\n",
        "                 flag_bn=True, \n",
        "                 name=\"Decoder\",\n",
        "                 **kwargs):\n",
        "        super(Decoder, self).__init__(name=name, **kwargs)\n",
        "        self.hidden_layer_dimensions = hidden_layer_dimensions\n",
        "        self.activation = activation\n",
        "        self.flag_bn = flag_bn\n",
        "        self.dense2 = Dense(hidden_layer_dimensions[2], activation=activation,\n",
        "                            name='decoder_2')\n",
        "        self.dense1 = Dense(hidden_layer_dimensions[1], activation=activation,\n",
        "                            name='decoder_1')\n",
        "        self.dense0 = Dense(hidden_layer_dimensions[0], activation=activation,\n",
        "                            name='decoder_0')\n",
        "        self.dense_output = Dense(inputs_dim, activation=activation,\n",
        "                            name='decoder_output')\n",
        "        if flag_bn:\n",
        "            self.batch_normalization2 = BatchNormalization(name=\"decoder_bn_layer_2\")\n",
        "            self.batch_normalization1 = BatchNormalization(name=\"decoder_bn_layer_1\")\n",
        "            self.batch_normalization0 = BatchNormalization(name=\"decoder_bn_layer_0\")\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Отображние z_rsr -> x_tilde, где x_tilde - вектор, лежащий в пространстве\n",
        "        исходных даных.\n",
        "        \"\"\"\n",
        "        z_rsr = inputs\n",
        "        z_rsr = self.dense2(z_rsr)\n",
        "        if self.flag_bn:\n",
        "            z_rsr = self.batch_normalization2(z_rsr)\n",
        "        z_rsr = self.dense1(z_rsr)\n",
        "        if self.flag_bn:\n",
        "            z_rsr = self.batch_normalization1(z_rsr)\n",
        "        z_rsr = self.dense0(z_rsr)\n",
        "        if self.flag_bn:\n",
        "            z_rsr = self.batch_normalization0(z_rsr)\n",
        "        x_tilde = self.dense_output(z_rsr)\n",
        "        return x_tilde\n",
        "    \n",
        "    # Опционально, пользовательский слой может быть сериализован реализацией метода \n",
        "    # get_config и метода класса (@classmethod) from_config.\n",
        "    def get_config(self):\n",
        "        config = super(Layer, self).get_config()\n",
        "        config.update({'hidden_layer_dimensions': self.hidden_layer_dimensions})\n",
        "        config.update({'activation': self.activation})\n",
        "        config.update({'flag_bn': self.flag_bn})\n",
        "        return config\n",
        "\n",
        "    # На самом деле нет необходимости определять `from_config` здесь, поскольку \n",
        "    # возвращение `cls(**config)` - поведение по умолчанию.\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "class RSRAE(Model):\n",
        "    \"\"\"\n",
        "    Нейросетевая модель-автоэнкодер для обнаружения аномалий с робастным слоем,\n",
        "    восстанавливающим подпространство (RSR layer между encoder и decoder).\n",
        "    Комбинируем encoder + RSR layer + decoder в end-to-end модель.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 inputs_dim, # размерность вектора признаков\n",
        "                 hidden_layer_dimensions,\n",
        "                 intrinsic_size, # разерность z_rsr после RSR layer\n",
        "                 activation,\n",
        "                 flag_bn=True,\n",
        "                 flag_normalize=True,\n",
        "                 learning_rate=1e-3,\n",
        "                 ae_loss_norm_type='MSE',\n",
        "                 rsr_loss_norm_type='MSE',\n",
        "                 name='RSRAE',\n",
        "                 **kwargs):\n",
        "        super(RSRAE, self).__init__(name=name, **kwargs)\n",
        "        self.inputs_dim = inputs_dim\n",
        "        self.hidden_layer_dimensions = hidden_layer_dimensions\n",
        "        self.intrinsic_size = intrinsic_size\n",
        "        self.activation = activation\n",
        "        self.flag_bn = flag_bn\n",
        "        self.flag_normalize = flag_normalize\n",
        "        self.learning_rate = learning_rate\n",
        "        self.ae_loss_norm_type = ae_loss_norm_type\n",
        "        self.rsr_loss_norm_type = rsr_loss_norm_type\n",
        "        # Для вычисления среднего loss по loss всех батчей в эпохе\n",
        "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
        "        self.auc_tracker = metrics.Mean(name=\"auc\")\n",
        "        self.ap_tracker = metrics.Mean(name=\"ap\")\n",
        "\n",
        "        # Создание экземпляров оптимизаторов\n",
        "        self.optimizer_ae = optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.optimizer_rsr1 = optimizers.Adam(learning_rate=10 * learning_rate)\n",
        "        self.optimizer_rsr2 = optimizers.Adam(learning_rate=10 * learning_rate)\n",
        "\n",
        "        # Слои\n",
        "        self.encoder = Encoder(hidden_layer_dimensions=hidden_layer_dimensions,\n",
        "                               activation=activation,\n",
        "                               flag_bn=flag_bn)\n",
        "        self.rsr = RSR(intrinsic_size=intrinsic_size)\n",
        "        if flag_normalize:\n",
        "            self.l2normalization = L2Normalization()\n",
        "        self.decoder = Decoder(inputs_dim=inputs_dim,\n",
        "                               hidden_layer_dimensions=hidden_layer_dimensions,\n",
        "                               activation=activation,\n",
        "                               flag_bn=flag_bn)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        z = self.encoder(inputs)\n",
        "        z_rsr = self.rsr(z)\n",
        "        if self.flag_normalize:\n",
        "            z_rsr = self.l2normalization(z_rsr)\n",
        "        x_tilde = self.decoder(z_rsr)\n",
        "        return z, z_rsr, x_tilde\n",
        "\n",
        "    def ae_loss(self, x, x_tilde):\n",
        "        \"\"\"Функция потерь реконструкции автоэнкодера - L_AE.\"\"\"\n",
        "\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "        x_tilde = tf.reshape(x_tilde, (tf.shape(x_tilde)[0], -1))\n",
        "\n",
        "        # axis=1 для tf.norm => вычисление вдоль оси признаков\n",
        "        # tf.math.reduce_mean без параметров - mean от элементов матрицы\n",
        "        if self.ae_loss_norm_type in ['MSE', 'mse', 'Frob', 'F']:\n",
        "            return tf.math.reduce_mean(tf.math.square(tf.norm(x-x_tilde, \n",
        "                                                              ord=2, axis=1)))\n",
        "        elif self.ae_loss_norm_type in ['L1', 'l1']:\n",
        "            return tf.math.reduce_mean(tf.norm(x-x_tilde, ord=1, axis=1))\n",
        "        elif self.ae_loss_norm_type in ['LAD', 'lad', 'L21', 'l21', 'L2', 'l2']:\n",
        "            return tf.math.reduce_mean(tf.norm(x-x_tilde, ord=2, axis=1))\n",
        "        else:\n",
        "            raise Exception(\"Norm type error!\")\n",
        "    \n",
        "    def rsr1_loss(self, z, z_rsr):\n",
        "        \"\"\"Функция потери для RSR layer - L_RSR1.\"\"\"\n",
        "        z_rsr = tf.matmul(z_rsr, tf.transpose(self.rsr.A))\n",
        "\n",
        "        if self.rsr_loss_norm_type in ['MSE', 'mse', 'Frob', 'F']:\n",
        "            return tf.math.reduce_mean(tf.math.square(tf.norm(z-z_rsr, ord=2, \n",
        "                                                            axis=1)))\n",
        "        elif self.rsr_loss_norm_type in ['L1', 'l1']:\n",
        "            return tf.math.reduce_mean(tf.norm(z-z_rsr, ord=1, axis=1))\n",
        "        elif self.rsr_loss_norm_type in ['LAD', 'lad', 'L21', 'l21', 'L2', 'l2']:\n",
        "            return tf.math.reduce_mean(tf.norm(z-z_rsr, ord=2, axis=1))\n",
        "        else:\n",
        "            raise Exception(\"Norm type error!\")\n",
        "    \n",
        "    def rsr2_loss(self):\n",
        "        \"\"\"Функция потери для RSR layer - L_RSR2.\"\"\"\n",
        "        A = self.rsr.A\n",
        "        A_T = tf.transpose(A)\n",
        "        I = tf.eye(self.intrinsic_size, dtype=tf.float64)\n",
        "        return tf.math.reduce_mean(tf.math.square(tf.linalg.matmul(A_T, A) - I))\n",
        "        \n",
        "    def gradients(model, inputs, targets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss_value = loss_fn(model, inputs, targets)\n",
        "        return tape.gradient(loss_value, model.trainable_variables)\n",
        "    \n",
        "    @tf.function()\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Override the method. Будет вызываться при 'model.fit()'.\n",
        "        Один шаг обучения, на котором вычисляются функции потерь для автоэнкодера и\n",
        "        RSR layer, и в соотвествии с ними обновляются значения обучаемых переменных - \n",
        "        весов нейросети и матрицы A соответсвенно. Будет вызываться от одного батча.\n",
        "        Заметим, что в этом методе мы используем пользовательские оптимизаторы и функции\n",
        "        потерь, поэтому перед тренировкой метод compile вызывать не придется.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        x, y = data\n",
        "\n",
        "        # tf.GradientTape() - записывает операции для автоматического дифференцирования\n",
        "\n",
        "        # По умолчанию persistent=False и удерживаемые GradientTape, высвобождаются,\n",
        "        # как только вызывается метод GradientTape.gradient(). Чтобы вычислить несколько\n",
        "        # градиентов за одно вычисление, требуется задать persistent=true. Это позволяет\n",
        "        # многократно вызывать метод gradient(), тогда требуется самостоятельно\n",
        "        # освободить ресурсы с помощью 'del tape'.\n",
        "\n",
        "        # watch_accessed_variables=True => автоматическое отслеживание всех обучаемых\n",
        "        # переменные, к которым осуществляется доступ. Так градиенты могут быть\n",
        "        # запрошены c любого вычисленного результата в tape.\n",
        "        with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tape:\n",
        "            # Здесь требуется запустить прямой проход нейросети. Операции применяемые\n",
        "            # при проходе к входных данным будут записаны на GradientTape. \n",
        "            z, z_rsr, x_tilde = self.call(x) # прямой проход RSRAE\n",
        "            z = tf.keras.layers.Flatten()(z) # вроде для текстовых данных необязательно\n",
        "            # Вычисляем значения функций потерь для этого прохода\n",
        "            loss_ae = self.ae_loss(x, x_tilde)\n",
        "            loss_rsr1 = self.rsr1_loss(z, z_rsr)\n",
        "            loss_rsr2 = self.rsr2_loss()\n",
        "  \n",
        "        # Метод gradient вычисляет градиенты обучаемых параметров(весов) для минимизации\n",
        "        # функции потерь, используя операции, записанные в контексте этого tape.\n",
        "        gradients_ae = tape.gradient(loss_ae, self.trainable_weights)\n",
        "        gradients_rsr1 = tape.gradient(loss_rsr1, self.rsr.A)\n",
        "        gradients_rsr2 = tape.gradient(loss_rsr2, self.rsr.A)\n",
        "\n",
        "        # Обновим значения обучаемых переменных - градиентный шаг чтобы min loss.\n",
        "        self.optimizer_ae.apply_gradients(grads_and_vars=\n",
        "                                          zip(gradients_ae, self.trainable_weights))\n",
        "        self.optimizer_rsr1.apply_gradients(grads_and_vars=\n",
        "                                            zip([gradients_rsr1], [self.rsr.A]))\n",
        "        self.optimizer_rsr2.apply_gradients(grads_and_vars=\n",
        "                                            zip([gradients_rsr2], [self.rsr.A]))\n",
        "        \n",
        "        self.loss_tracker.update_state(loss_ae) # обновляем средний loss по батчам\n",
        "\n",
        "        # Обновляем метрики\n",
        "        if len(tf.unique(y)[0]) == 2:\n",
        "            # иначе roc_auc_score бросит ValueError и обучение приостановится\n",
        "            auc = self.auc_metric(y, cosine_similarity(x_tilde, x))\n",
        "            self.auc_tracker.update_state(auc)\n",
        "\n",
        "        ap = self.ap_metric(y, cosine_similarity(x_tilde, x))\n",
        "        self.ap_tracker.update_state(ap)\n",
        "\n",
        "        del tape # persistent=True => требуется самостоятельно освободить ресурсы\n",
        "        return {\"loss\": self.loss_tracker.result(),\n",
        "                \"auc\": self.auc_tracker.result(),\n",
        "                \"ap\": self.ap_tracker.result(),}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        \"\"\"\n",
        "        В пару к train_step. Сбрасывает метрики (`reset_states()`) в начале каждой\n",
        "        эпохи обучения с помощью 'fit()'. Без этого свойства 'result()' будет \n",
        "        возвращать среднее значение с начала обучения.\n",
        "        \"\"\"\n",
        "        return [self.loss_tracker, self.auc_tracker, self.ap_tracker]\n",
        "\n",
        "    def auc_metric(self, y_true, y_pred):\n",
        "        return tf.py_function(roc_auc_score, (y_true, y_pred), tf.float64)\n",
        "\n",
        "    def ap_metric(self, y_true, y_pred):\n",
        "        return tf.py_function(average_precision_score, (y_true, y_pred), tf.float64)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version = 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5KyJfmtvFLq",
        "outputId": "10af3d4f-d49f-43b9-ba44-bcf3311c2257"
      },
      "source": [
        "model_rsrae = RSRAE(inputs_dim=x.shape[-1],\n",
        "                    hidden_layer_dimensions=[128, 256, 512],\n",
        "                    intrinsic_size=50,\n",
        "                    activation='relu',\n",
        "                    learning_rate=25e-5,\n",
        "                    ae_loss_norm_type='MSE',\n",
        "                 rsr_loss_norm_type='MSE',)\n",
        "model_rsrae.compile(run_eagerly=True)\n",
        "model_rsrae.fit(x, y,\n",
        "                batch_size=128,\n",
        "                epochs=500)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "9/9 [==============================] - 1s 107ms/step - loss: 1.0096 - auc: 0.5416 - ap: 0.1787\n",
            "Epoch 2/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.9891 - auc: 0.5220 - ap: 0.1565\n",
            "Epoch 3/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.9852 - auc: 0.5605 - ap: 0.1889\n",
            "Epoch 4/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.9836 - auc: 0.5958 - ap: 0.2151\n",
            "Epoch 5/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.9824 - auc: 0.6268 - ap: 0.2585\n",
            "Epoch 6/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.9812 - auc: 0.6424 - ap: 0.2720\n",
            "Epoch 7/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.9797 - auc: 0.6695 - ap: 0.2970\n",
            "Epoch 8/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.9775 - auc: 0.6708 - ap: 0.2859\n",
            "Epoch 9/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.9744 - auc: 0.6819 - ap: 0.2961\n",
            "Epoch 10/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.9702 - auc: 0.6814 - ap: 0.2770\n",
            "Epoch 11/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.9644 - auc: 0.6672 - ap: 0.2611\n",
            "Epoch 12/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.9571 - auc: 0.5974 - ap: 0.2028\n",
            "Epoch 13/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.9478 - auc: 0.5412 - ap: 0.1984\n",
            "Epoch 14/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.9373 - auc: 0.5073 - ap: 0.1875\n",
            "Epoch 15/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.9247 - auc: 0.4891 - ap: 0.1807\n",
            "Epoch 16/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.9103 - auc: 0.5243 - ap: 0.1999\n",
            "Epoch 17/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.8961 - auc: 0.5009 - ap: 0.1961\n",
            "Epoch 18/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.8805 - auc: 0.5307 - ap: 0.2186\n",
            "Epoch 19/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.8639 - auc: 0.5327 - ap: 0.2022\n",
            "Epoch 20/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.8470 - auc: 0.5531 - ap: 0.2225\n",
            "Epoch 21/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.8297 - auc: 0.5601 - ap: 0.2048\n",
            "Epoch 22/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.8134 - auc: 0.5766 - ap: 0.2205\n",
            "Epoch 23/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.7971 - auc: 0.5882 - ap: 0.2164\n",
            "Epoch 24/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.7816 - auc: 0.5926 - ap: 0.2212\n",
            "Epoch 25/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.7675 - auc: 0.5944 - ap: 0.2282\n",
            "Epoch 26/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.7539 - auc: 0.5930 - ap: 0.2260\n",
            "Epoch 27/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.7418 - auc: 0.6067 - ap: 0.2423\n",
            "Epoch 28/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.7305 - auc: 0.6028 - ap: 0.2353\n",
            "Epoch 29/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.7197 - auc: 0.6224 - ap: 0.2530\n",
            "Epoch 30/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.7098 - auc: 0.6212 - ap: 0.2360\n",
            "Epoch 31/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.7020 - auc: 0.6103 - ap: 0.2442\n",
            "Epoch 32/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6945 - auc: 0.6278 - ap: 0.2363\n",
            "Epoch 33/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6883 - auc: 0.6253 - ap: 0.2628\n",
            "Epoch 34/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6817 - auc: 0.6387 - ap: 0.2509\n",
            "Epoch 35/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6773 - auc: 0.6261 - ap: 0.2563\n",
            "Epoch 36/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6720 - auc: 0.6440 - ap: 0.2497\n",
            "Epoch 37/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6666 - auc: 0.6088 - ap: 0.2315\n",
            "Epoch 38/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6615 - auc: 0.6267 - ap: 0.2498\n",
            "Epoch 39/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6578 - auc: 0.6205 - ap: 0.2501\n",
            "Epoch 40/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6529 - auc: 0.6542 - ap: 0.2650\n",
            "Epoch 41/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6500 - auc: 0.6129 - ap: 0.2458\n",
            "Epoch 42/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6472 - auc: 0.6433 - ap: 0.2542\n",
            "Epoch 43/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6445 - auc: 0.6147 - ap: 0.2309\n",
            "Epoch 44/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6435 - auc: 0.6291 - ap: 0.2515\n",
            "Epoch 45/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6411 - auc: 0.6342 - ap: 0.2599\n",
            "Epoch 46/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6401 - auc: 0.6554 - ap: 0.2488\n",
            "Epoch 47/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6386 - auc: 0.6165 - ap: 0.2412\n",
            "Epoch 48/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6367 - auc: 0.6474 - ap: 0.2746\n",
            "Epoch 49/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6351 - auc: 0.6338 - ap: 0.2500\n",
            "Epoch 50/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6320 - auc: 0.6481 - ap: 0.2668\n",
            "Epoch 51/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6295 - auc: 0.6226 - ap: 0.2651\n",
            "Epoch 52/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6282 - auc: 0.6403 - ap: 0.2483\n",
            "Epoch 53/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6269 - auc: 0.6537 - ap: 0.2722\n",
            "Epoch 54/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6259 - auc: 0.6432 - ap: 0.2597\n",
            "Epoch 55/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6253 - auc: 0.6523 - ap: 0.2560\n",
            "Epoch 56/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6241 - auc: 0.6379 - ap: 0.2571\n",
            "Epoch 57/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6237 - auc: 0.6620 - ap: 0.2702\n",
            "Epoch 58/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6229 - auc: 0.6405 - ap: 0.2624\n",
            "Epoch 59/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6221 - auc: 0.6603 - ap: 0.2660\n",
            "Epoch 60/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6223 - auc: 0.6320 - ap: 0.2577\n",
            "Epoch 61/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6235 - auc: 0.6529 - ap: 0.2718\n",
            "Epoch 62/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6225 - auc: 0.6502 - ap: 0.2820\n",
            "Epoch 63/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6210 - auc: 0.6537 - ap: 0.2722\n",
            "Epoch 64/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6207 - auc: 0.6393 - ap: 0.2592\n",
            "Epoch 65/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6203 - auc: 0.6491 - ap: 0.2525\n",
            "Epoch 66/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6203 - auc: 0.6505 - ap: 0.2675\n",
            "Epoch 67/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6196 - auc: 0.6648 - ap: 0.2744\n",
            "Epoch 68/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6199 - auc: 0.6625 - ap: 0.2727\n",
            "Epoch 69/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6190 - auc: 0.6512 - ap: 0.2698\n",
            "Epoch 70/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6193 - auc: 0.6468 - ap: 0.2645\n",
            "Epoch 71/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6183 - auc: 0.6576 - ap: 0.2771\n",
            "Epoch 72/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6173 - auc: 0.6320 - ap: 0.2628\n",
            "Epoch 73/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6167 - auc: 0.6549 - ap: 0.2669\n",
            "Epoch 74/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6164 - auc: 0.6530 - ap: 0.2712\n",
            "Epoch 75/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6167 - auc: 0.6634 - ap: 0.2826\n",
            "Epoch 76/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6147 - auc: 0.6672 - ap: 0.2655\n",
            "Epoch 77/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6146 - auc: 0.6424 - ap: 0.2651\n",
            "Epoch 78/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6140 - auc: 0.6579 - ap: 0.2729\n",
            "Epoch 79/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6136 - auc: 0.6508 - ap: 0.2682\n",
            "Epoch 80/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6130 - auc: 0.6746 - ap: 0.2841\n",
            "Epoch 81/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6135 - auc: 0.6514 - ap: 0.2891\n",
            "Epoch 82/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6136 - auc: 0.6671 - ap: 0.2628\n",
            "Epoch 83/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6135 - auc: 0.6482 - ap: 0.2697\n",
            "Epoch 84/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6135 - auc: 0.6569 - ap: 0.2744\n",
            "Epoch 85/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6135 - auc: 0.6625 - ap: 0.2917\n",
            "Epoch 86/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6129 - auc: 0.6635 - ap: 0.2623\n",
            "Epoch 87/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6126 - auc: 0.6510 - ap: 0.2847\n",
            "Epoch 88/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6134 - auc: 0.6676 - ap: 0.2730\n",
            "Epoch 89/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6136 - auc: 0.6656 - ap: 0.2740\n",
            "Epoch 90/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6132 - auc: 0.6449 - ap: 0.2429\n",
            "Epoch 91/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6135 - auc: 0.6648 - ap: 0.2983\n",
            "Epoch 92/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6127 - auc: 0.6616 - ap: 0.2765\n",
            "Epoch 93/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6128 - auc: 0.6502 - ap: 0.2744\n",
            "Epoch 94/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6122 - auc: 0.6590 - ap: 0.2944\n",
            "Epoch 95/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6113 - auc: 0.6596 - ap: 0.2672\n",
            "Epoch 96/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6112 - auc: 0.6563 - ap: 0.2550\n",
            "Epoch 97/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6114 - auc: 0.6566 - ap: 0.2770\n",
            "Epoch 98/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6115 - auc: 0.6586 - ap: 0.2794\n",
            "Epoch 99/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6119 - auc: 0.6770 - ap: 0.2804\n",
            "Epoch 100/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6112 - auc: 0.6545 - ap: 0.2705\n",
            "Epoch 101/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6114 - auc: 0.6601 - ap: 0.2804\n",
            "Epoch 102/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6116 - auc: 0.6594 - ap: 0.2744\n",
            "Epoch 103/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6121 - auc: 0.6614 - ap: 0.2687\n",
            "Epoch 104/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6125 - auc: 0.6593 - ap: 0.2861\n",
            "Epoch 105/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6125 - auc: 0.6665 - ap: 0.2726\n",
            "Epoch 106/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6117 - auc: 0.6516 - ap: 0.2734\n",
            "Epoch 107/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6111 - auc: 0.6598 - ap: 0.2655\n",
            "Epoch 108/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6109 - auc: 0.6551 - ap: 0.2643\n",
            "Epoch 109/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6112 - auc: 0.6671 - ap: 0.2817\n",
            "Epoch 110/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6106 - auc: 0.6691 - ap: 0.2690\n",
            "Epoch 111/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6105 - auc: 0.6802 - ap: 0.2920\n",
            "Epoch 112/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6102 - auc: 0.6406 - ap: 0.2572\n",
            "Epoch 113/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6101 - auc: 0.6737 - ap: 0.2807\n",
            "Epoch 114/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6101 - auc: 0.6535 - ap: 0.2696\n",
            "Epoch 115/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6096 - auc: 0.6654 - ap: 0.2597\n",
            "Epoch 116/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6097 - auc: 0.6596 - ap: 0.2785\n",
            "Epoch 117/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6095 - auc: 0.6548 - ap: 0.2853\n",
            "Epoch 118/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6091 - auc: 0.6609 - ap: 0.2723\n",
            "Epoch 119/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6094 - auc: 0.6595 - ap: 0.2667\n",
            "Epoch 120/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6093 - auc: 0.6549 - ap: 0.2690\n",
            "Epoch 121/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6098 - auc: 0.6567 - ap: 0.2749\n",
            "Epoch 122/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6096 - auc: 0.6644 - ap: 0.2872\n",
            "Epoch 123/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6098 - auc: 0.6556 - ap: 0.2822\n",
            "Epoch 124/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6097 - auc: 0.6630 - ap: 0.2793\n",
            "Epoch 125/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6102 - auc: 0.6547 - ap: 0.2782\n",
            "Epoch 126/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6097 - auc: 0.6508 - ap: 0.2484\n",
            "Epoch 127/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6094 - auc: 0.6491 - ap: 0.2745\n",
            "Epoch 128/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6089 - auc: 0.6627 - ap: 0.2660\n",
            "Epoch 129/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6090 - auc: 0.6800 - ap: 0.2924\n",
            "Epoch 130/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6090 - auc: 0.6606 - ap: 0.2739\n",
            "Epoch 131/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6089 - auc: 0.6630 - ap: 0.2656\n",
            "Epoch 132/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6088 - auc: 0.6594 - ap: 0.2733\n",
            "Epoch 133/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6089 - auc: 0.6659 - ap: 0.2726\n",
            "Epoch 134/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6084 - auc: 0.6488 - ap: 0.2800\n",
            "Epoch 135/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6092 - auc: 0.6650 - ap: 0.2711\n",
            "Epoch 136/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6088 - auc: 0.6684 - ap: 0.2875\n",
            "Epoch 137/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6089 - auc: 0.6737 - ap: 0.2858\n",
            "Epoch 138/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6090 - auc: 0.6586 - ap: 0.2939\n",
            "Epoch 139/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6086 - auc: 0.6657 - ap: 0.2761\n",
            "Epoch 140/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6077 - auc: 0.6634 - ap: 0.2852\n",
            "Epoch 141/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6075 - auc: 0.6615 - ap: 0.2697\n",
            "Epoch 142/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6080 - auc: 0.6551 - ap: 0.2817\n",
            "Epoch 143/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6085 - auc: 0.6655 - ap: 0.2725\n",
            "Epoch 144/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6082 - auc: 0.6572 - ap: 0.2800\n",
            "Epoch 145/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6074 - auc: 0.6488 - ap: 0.2653\n",
            "Epoch 146/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6077 - auc: 0.6664 - ap: 0.2842\n",
            "Epoch 147/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6080 - auc: 0.6594 - ap: 0.2776\n",
            "Epoch 148/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6080 - auc: 0.6498 - ap: 0.2716\n",
            "Epoch 149/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6080 - auc: 0.6642 - ap: 0.2607\n",
            "Epoch 150/500\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.6082 - auc: 0.6486 - ap: 0.2750\n",
            "Epoch 151/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6083 - auc: 0.6762 - ap: 0.2785\n",
            "Epoch 152/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6087 - auc: 0.6505 - ap: 0.2639\n",
            "Epoch 153/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6079 - auc: 0.6540 - ap: 0.2600\n",
            "Epoch 154/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6083 - auc: 0.6615 - ap: 0.2766\n",
            "Epoch 155/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6081 - auc: 0.6667 - ap: 0.2950\n",
            "Epoch 156/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6077 - auc: 0.6670 - ap: 0.2642\n",
            "Epoch 157/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6077 - auc: 0.6557 - ap: 0.2752\n",
            "Epoch 158/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6080 - auc: 0.6569 - ap: 0.2684\n",
            "Epoch 159/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6082 - auc: 0.6761 - ap: 0.2775\n",
            "Epoch 160/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6074 - auc: 0.6588 - ap: 0.2881\n",
            "Epoch 161/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6069 - auc: 0.6559 - ap: 0.2657\n",
            "Epoch 162/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6070 - auc: 0.6771 - ap: 0.2780\n",
            "Epoch 163/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6069 - auc: 0.6452 - ap: 0.2640\n",
            "Epoch 164/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6073 - auc: 0.6696 - ap: 0.2715\n",
            "Epoch 165/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6075 - auc: 0.6633 - ap: 0.2994\n",
            "Epoch 166/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6073 - auc: 0.6568 - ap: 0.2808\n",
            "Epoch 167/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6076 - auc: 0.6545 - ap: 0.2679\n",
            "Epoch 168/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6078 - auc: 0.6562 - ap: 0.2708\n",
            "Epoch 169/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6078 - auc: 0.6635 - ap: 0.2670\n",
            "Epoch 170/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6075 - auc: 0.6674 - ap: 0.2684\n",
            "Epoch 171/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6070 - auc: 0.6714 - ap: 0.3028\n",
            "Epoch 172/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6073 - auc: 0.6689 - ap: 0.2874\n",
            "Epoch 173/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6073 - auc: 0.6521 - ap: 0.2684\n",
            "Epoch 174/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6069 - auc: 0.6511 - ap: 0.2586\n",
            "Epoch 175/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6073 - auc: 0.6638 - ap: 0.2723\n",
            "Epoch 176/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6069 - auc: 0.6661 - ap: 0.2766\n",
            "Epoch 177/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6072 - auc: 0.6592 - ap: 0.2748\n",
            "Epoch 178/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6071 - auc: 0.6578 - ap: 0.2948\n",
            "Epoch 179/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6064 - auc: 0.6638 - ap: 0.2809\n",
            "Epoch 180/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6065 - auc: 0.6559 - ap: 0.2709\n",
            "Epoch 181/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6065 - auc: 0.6629 - ap: 0.2695\n",
            "Epoch 182/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6070 - auc: 0.6605 - ap: 0.2728\n",
            "Epoch 183/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6057 - auc: 0.6577 - ap: 0.2766\n",
            "Epoch 184/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6065 - auc: 0.6549 - ap: 0.2738\n",
            "Epoch 185/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6060 - auc: 0.6703 - ap: 0.2616\n",
            "Epoch 186/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6065 - auc: 0.6530 - ap: 0.2730\n",
            "Epoch 187/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6062 - auc: 0.6666 - ap: 0.2979\n",
            "Epoch 188/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6056 - auc: 0.6538 - ap: 0.2528\n",
            "Epoch 189/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6061 - auc: 0.6611 - ap: 0.2801\n",
            "Epoch 190/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6062 - auc: 0.6495 - ap: 0.2672\n",
            "Epoch 191/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6061 - auc: 0.6701 - ap: 0.2721\n",
            "Epoch 192/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6067 - auc: 0.6538 - ap: 0.2651\n",
            "Epoch 193/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6064 - auc: 0.6664 - ap: 0.2695\n",
            "Epoch 194/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6066 - auc: 0.6603 - ap: 0.2684\n",
            "Epoch 195/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6064 - auc: 0.6605 - ap: 0.2759\n",
            "Epoch 196/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6060 - auc: 0.6571 - ap: 0.2809\n",
            "Epoch 197/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6065 - auc: 0.6637 - ap: 0.2815\n",
            "Epoch 198/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6057 - auc: 0.6467 - ap: 0.2720\n",
            "Epoch 199/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6056 - auc: 0.6636 - ap: 0.2905\n",
            "Epoch 200/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6063 - auc: 0.6510 - ap: 0.2729\n",
            "Epoch 201/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6058 - auc: 0.6633 - ap: 0.2717\n",
            "Epoch 202/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6059 - auc: 0.6543 - ap: 0.2629\n",
            "Epoch 203/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6059 - auc: 0.6719 - ap: 0.2742\n",
            "Epoch 204/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6052 - auc: 0.6570 - ap: 0.2682\n",
            "Epoch 205/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6060 - auc: 0.6750 - ap: 0.2899\n",
            "Epoch 206/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6064 - auc: 0.6444 - ap: 0.2903\n",
            "Epoch 207/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6063 - auc: 0.6791 - ap: 0.2667\n",
            "Epoch 208/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6060 - auc: 0.6553 - ap: 0.2814\n",
            "Epoch 209/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6060 - auc: 0.6699 - ap: 0.2735\n",
            "Epoch 210/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6056 - auc: 0.6557 - ap: 0.2715\n",
            "Epoch 211/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6059 - auc: 0.6809 - ap: 0.2884\n",
            "Epoch 212/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6062 - auc: 0.6551 - ap: 0.2702\n",
            "Epoch 213/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6059 - auc: 0.6700 - ap: 0.2859\n",
            "Epoch 214/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6061 - auc: 0.6587 - ap: 0.2723\n",
            "Epoch 215/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6064 - auc: 0.6714 - ap: 0.2886\n",
            "Epoch 216/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6062 - auc: 0.6582 - ap: 0.2636\n",
            "Epoch 217/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6058 - auc: 0.6628 - ap: 0.2810\n",
            "Epoch 218/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6061 - auc: 0.6515 - ap: 0.2634\n",
            "Epoch 219/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6054 - auc: 0.6678 - ap: 0.2590\n",
            "Epoch 220/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6055 - auc: 0.6434 - ap: 0.2726\n",
            "Epoch 221/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6058 - auc: 0.6734 - ap: 0.2603\n",
            "Epoch 222/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6062 - auc: 0.6481 - ap: 0.2716\n",
            "Epoch 223/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6056 - auc: 0.6681 - ap: 0.2755\n",
            "Epoch 224/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6064 - auc: 0.6670 - ap: 0.2756\n",
            "Epoch 225/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6063 - auc: 0.6570 - ap: 0.2735\n",
            "Epoch 226/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6060 - auc: 0.6523 - ap: 0.2583\n",
            "Epoch 227/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6060 - auc: 0.6690 - ap: 0.2899\n",
            "Epoch 228/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6058 - auc: 0.6559 - ap: 0.2677\n",
            "Epoch 229/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6053 - auc: 0.6693 - ap: 0.2745\n",
            "Epoch 230/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6060 - auc: 0.6570 - ap: 0.2596\n",
            "Epoch 231/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6058 - auc: 0.6618 - ap: 0.2813\n",
            "Epoch 232/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6053 - auc: 0.6487 - ap: 0.2656\n",
            "Epoch 233/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6055 - auc: 0.6568 - ap: 0.2674\n",
            "Epoch 234/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6057 - auc: 0.6702 - ap: 0.2732\n",
            "Epoch 235/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6055 - auc: 0.6568 - ap: 0.2718\n",
            "Epoch 236/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6052 - auc: 0.6485 - ap: 0.2709\n",
            "Epoch 237/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6053 - auc: 0.6757 - ap: 0.2870\n",
            "Epoch 238/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6051 - auc: 0.6635 - ap: 0.2832\n",
            "Epoch 239/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6052 - auc: 0.6517 - ap: 0.2711\n",
            "Epoch 240/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6055 - auc: 0.6673 - ap: 0.2694\n",
            "Epoch 241/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6052 - auc: 0.6568 - ap: 0.2683\n",
            "Epoch 242/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6049 - auc: 0.6618 - ap: 0.2826\n",
            "Epoch 243/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6049 - auc: 0.6653 - ap: 0.2682\n",
            "Epoch 244/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6053 - auc: 0.6481 - ap: 0.2627\n",
            "Epoch 245/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6054 - auc: 0.6626 - ap: 0.2733\n",
            "Epoch 246/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6056 - auc: 0.6607 - ap: 0.2745\n",
            "Epoch 247/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6053 - auc: 0.6623 - ap: 0.2601\n",
            "Epoch 248/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6058 - auc: 0.6738 - ap: 0.2710\n",
            "Epoch 249/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6061 - auc: 0.6641 - ap: 0.2760\n",
            "Epoch 250/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6047 - auc: 0.6526 - ap: 0.2887\n",
            "Epoch 251/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6055 - auc: 0.6568 - ap: 0.2689\n",
            "Epoch 252/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6054 - auc: 0.6565 - ap: 0.2743\n",
            "Epoch 253/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6049 - auc: 0.6649 - ap: 0.2703\n",
            "Epoch 254/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6055 - auc: 0.6526 - ap: 0.2654\n",
            "Epoch 255/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6057 - auc: 0.6705 - ap: 0.2770\n",
            "Epoch 256/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6053 - auc: 0.6522 - ap: 0.2529\n",
            "Epoch 257/500\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.6056 - auc: 0.6622 - ap: 0.2841\n",
            "Epoch 258/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6055 - auc: 0.6505 - ap: 0.2797\n",
            "Epoch 259/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6052 - auc: 0.6526 - ap: 0.2811\n",
            "Epoch 260/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6051 - auc: 0.6547 - ap: 0.2933\n",
            "Epoch 261/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6054 - auc: 0.6610 - ap: 0.2720\n",
            "Epoch 262/500\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.6051 - auc: 0.6704 - ap: 0.2736\n",
            "Epoch 263/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6052 - auc: 0.6625 - ap: 0.2840\n",
            "Epoch 264/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6049 - auc: 0.6618 - ap: 0.2936\n",
            "Epoch 265/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6047 - auc: 0.6550 - ap: 0.2816\n",
            "Epoch 266/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6047 - auc: 0.6549 - ap: 0.2759\n",
            "Epoch 267/500\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.6049 - auc: 0.6486 - ap: 0.2641\n",
            "Epoch 268/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6046 - auc: 0.6657 - ap: 0.2784\n",
            "Epoch 269/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6047 - auc: 0.6636 - ap: 0.2777\n",
            "Epoch 270/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6044 - auc: 0.6642 - ap: 0.2752\n",
            "Epoch 271/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6043 - auc: 0.6679 - ap: 0.2689\n",
            "Epoch 272/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6049 - auc: 0.6515 - ap: 0.2663\n",
            "Epoch 273/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6042 - auc: 0.6632 - ap: 0.2811\n",
            "Epoch 274/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6045 - auc: 0.6618 - ap: 0.2643\n",
            "Epoch 275/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6042 - auc: 0.6650 - ap: 0.2730\n",
            "Epoch 276/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6045 - auc: 0.6634 - ap: 0.2720\n",
            "Epoch 277/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6043 - auc: 0.6555 - ap: 0.2742\n",
            "Epoch 278/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6054 - auc: 0.6572 - ap: 0.2671\n",
            "Epoch 279/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6045 - auc: 0.6599 - ap: 0.2800\n",
            "Epoch 280/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6051 - auc: 0.6524 - ap: 0.2691\n",
            "Epoch 281/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6044 - auc: 0.6680 - ap: 0.2789\n",
            "Epoch 282/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6045 - auc: 0.6547 - ap: 0.2664\n",
            "Epoch 283/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6045 - auc: 0.6682 - ap: 0.2765\n",
            "Epoch 284/500\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.6040 - auc: 0.6521 - ap: 0.2649\n",
            "Epoch 285/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6044 - auc: 0.6622 - ap: 0.2651\n",
            "Epoch 286/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6042 - auc: 0.6583 - ap: 0.2711\n",
            "Epoch 287/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6042 - auc: 0.6518 - ap: 0.2690\n",
            "Epoch 288/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6042 - auc: 0.6659 - ap: 0.2654\n",
            "Epoch 289/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6053 - auc: 0.6631 - ap: 0.2604\n",
            "Epoch 290/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6048 - auc: 0.6581 - ap: 0.2751\n",
            "Epoch 291/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6042 - auc: 0.6652 - ap: 0.2787\n",
            "Epoch 292/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6045 - auc: 0.6621 - ap: 0.2723\n",
            "Epoch 293/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6045 - auc: 0.6609 - ap: 0.2741\n",
            "Epoch 294/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6043 - auc: 0.6590 - ap: 0.2709\n",
            "Epoch 295/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6043 - auc: 0.6555 - ap: 0.2894\n",
            "Epoch 296/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6040 - auc: 0.6662 - ap: 0.2683\n",
            "Epoch 297/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6045 - auc: 0.6677 - ap: 0.3072\n",
            "Epoch 298/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6051 - auc: 0.6562 - ap: 0.2762\n",
            "Epoch 299/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6046 - auc: 0.6636 - ap: 0.2792\n",
            "Epoch 300/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6047 - auc: 0.6583 - ap: 0.2806\n",
            "Epoch 301/500\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.6050 - auc: 0.6664 - ap: 0.2661\n",
            "Epoch 302/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6049 - auc: 0.6592 - ap: 0.2787\n",
            "Epoch 303/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6047 - auc: 0.6661 - ap: 0.2700\n",
            "Epoch 304/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6046 - auc: 0.6707 - ap: 0.2839\n",
            "Epoch 305/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6046 - auc: 0.6643 - ap: 0.2607\n",
            "Epoch 306/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6045 - auc: 0.6631 - ap: 0.2774\n",
            "Epoch 307/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6043 - auc: 0.6620 - ap: 0.2847\n",
            "Epoch 308/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6047 - auc: 0.6659 - ap: 0.2769\n",
            "Epoch 309/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6038 - auc: 0.6530 - ap: 0.2770\n",
            "Epoch 310/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6043 - auc: 0.6618 - ap: 0.2720\n",
            "Epoch 311/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6038 - auc: 0.6611 - ap: 0.2753\n",
            "Epoch 312/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6041 - auc: 0.6651 - ap: 0.2788\n",
            "Epoch 313/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6042 - auc: 0.6556 - ap: 0.2827\n",
            "Epoch 314/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6041 - auc: 0.6634 - ap: 0.2752\n",
            "Epoch 315/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6044 - auc: 0.6509 - ap: 0.2710\n",
            "Epoch 316/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6041 - auc: 0.6848 - ap: 0.2853\n",
            "Epoch 317/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6044 - auc: 0.6459 - ap: 0.2705\n",
            "Epoch 318/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6041 - auc: 0.6586 - ap: 0.2882\n",
            "Epoch 319/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6043 - auc: 0.6598 - ap: 0.2720\n",
            "Epoch 320/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6046 - auc: 0.6688 - ap: 0.2735\n",
            "Epoch 321/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6042 - auc: 0.6558 - ap: 0.2576\n",
            "Epoch 322/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6044 - auc: 0.6719 - ap: 0.2822\n",
            "Epoch 323/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6040 - auc: 0.6573 - ap: 0.2732\n",
            "Epoch 324/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6042 - auc: 0.6585 - ap: 0.2708\n",
            "Epoch 325/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6040 - auc: 0.6620 - ap: 0.2626\n",
            "Epoch 326/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6038 - auc: 0.6558 - ap: 0.2675\n",
            "Epoch 327/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6042 - auc: 0.6604 - ap: 0.2584\n",
            "Epoch 328/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6041 - auc: 0.6666 - ap: 0.2811\n",
            "Epoch 329/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6040 - auc: 0.6493 - ap: 0.2576\n",
            "Epoch 330/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6047 - auc: 0.6629 - ap: 0.2792\n",
            "Epoch 331/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6046 - auc: 0.6639 - ap: 0.2827\n",
            "Epoch 332/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6044 - auc: 0.6596 - ap: 0.2846\n",
            "Epoch 333/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6044 - auc: 0.6633 - ap: 0.2751\n",
            "Epoch 334/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6045 - auc: 0.6579 - ap: 0.2704\n",
            "Epoch 335/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6045 - auc: 0.6608 - ap: 0.2736\n",
            "Epoch 336/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6038 - auc: 0.6731 - ap: 0.3054\n",
            "Epoch 337/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6043 - auc: 0.6645 - ap: 0.2718\n",
            "Epoch 338/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6043 - auc: 0.6575 - ap: 0.2744\n",
            "Epoch 339/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6040 - auc: 0.6620 - ap: 0.2794\n",
            "Epoch 340/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6038 - auc: 0.6655 - ap: 0.2727\n",
            "Epoch 341/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6046 - auc: 0.6534 - ap: 0.2622\n",
            "Epoch 342/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6043 - auc: 0.6684 - ap: 0.2811\n",
            "Epoch 343/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6045 - auc: 0.6491 - ap: 0.2632\n",
            "Epoch 344/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6041 - auc: 0.6715 - ap: 0.2811\n",
            "Epoch 345/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6041 - auc: 0.6552 - ap: 0.2675\n",
            "Epoch 346/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6042 - auc: 0.6615 - ap: 0.2772\n",
            "Epoch 347/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6032 - auc: 0.6570 - ap: 0.2595\n",
            "Epoch 348/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6037 - auc: 0.6677 - ap: 0.3028\n",
            "Epoch 349/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6043 - auc: 0.6594 - ap: 0.2832\n",
            "Epoch 350/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6036 - auc: 0.6586 - ap: 0.2670\n",
            "Epoch 351/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6043 - auc: 0.6628 - ap: 0.2767\n",
            "Epoch 352/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6042 - auc: 0.6627 - ap: 0.2745\n",
            "Epoch 353/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6041 - auc: 0.6633 - ap: 0.2786\n",
            "Epoch 354/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6035 - auc: 0.6640 - ap: 0.2739\n",
            "Epoch 355/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6034 - auc: 0.6573 - ap: 0.2674\n",
            "Epoch 356/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6039 - auc: 0.6554 - ap: 0.2765\n",
            "Epoch 357/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6041 - auc: 0.6717 - ap: 0.2864\n",
            "Epoch 358/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6039 - auc: 0.6765 - ap: 0.2824\n",
            "Epoch 359/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6042 - auc: 0.6592 - ap: 0.2686\n",
            "Epoch 360/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6040 - auc: 0.6596 - ap: 0.2759\n",
            "Epoch 361/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6037 - auc: 0.6614 - ap: 0.2759\n",
            "Epoch 362/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6038 - auc: 0.6559 - ap: 0.2646\n",
            "Epoch 363/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6044 - auc: 0.6683 - ap: 0.2721\n",
            "Epoch 364/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6036 - auc: 0.6631 - ap: 0.2884\n",
            "Epoch 365/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6039 - auc: 0.6664 - ap: 0.2887\n",
            "Epoch 366/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6045 - auc: 0.6550 - ap: 0.2774\n",
            "Epoch 367/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6043 - auc: 0.6544 - ap: 0.2614\n",
            "Epoch 368/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6041 - auc: 0.6705 - ap: 0.2902\n",
            "Epoch 369/500\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.6046 - auc: 0.6498 - ap: 0.2813\n",
            "Epoch 370/500\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.6040 - auc: 0.6656 - ap: 0.2885\n",
            "Epoch 371/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6040 - auc: 0.6614 - ap: 0.2901\n",
            "Epoch 372/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6045 - auc: 0.6814 - ap: 0.2771\n",
            "Epoch 373/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6041 - auc: 0.6557 - ap: 0.2931\n",
            "Epoch 374/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6037 - auc: 0.6696 - ap: 0.2758\n",
            "Epoch 375/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6037 - auc: 0.6505 - ap: 0.2663\n",
            "Epoch 376/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6039 - auc: 0.6708 - ap: 0.2899\n",
            "Epoch 377/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6040 - auc: 0.6683 - ap: 0.2812\n",
            "Epoch 378/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6040 - auc: 0.6627 - ap: 0.2743\n",
            "Epoch 379/500\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.6041 - auc: 0.6584 - ap: 0.2796\n",
            "Epoch 380/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6044 - auc: 0.6640 - ap: 0.2819\n",
            "Epoch 381/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6046 - auc: 0.6536 - ap: 0.2742\n",
            "Epoch 382/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6039 - auc: 0.6607 - ap: 0.2762\n",
            "Epoch 383/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6038 - auc: 0.6532 - ap: 0.2758\n",
            "Epoch 384/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6039 - auc: 0.6608 - ap: 0.2835\n",
            "Epoch 385/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6034 - auc: 0.6595 - ap: 0.2846\n",
            "Epoch 386/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6039 - auc: 0.6614 - ap: 0.2634\n",
            "Epoch 387/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6036 - auc: 0.6557 - ap: 0.2845\n",
            "Epoch 388/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6042 - auc: 0.6622 - ap: 0.2780\n",
            "Epoch 389/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6036 - auc: 0.6478 - ap: 0.2596\n",
            "Epoch 390/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6041 - auc: 0.6524 - ap: 0.2768\n",
            "Epoch 391/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6037 - auc: 0.6639 - ap: 0.2720\n",
            "Epoch 392/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6038 - auc: 0.6574 - ap: 0.2818\n",
            "Epoch 393/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6039 - auc: 0.6564 - ap: 0.2733\n",
            "Epoch 394/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6042 - auc: 0.6743 - ap: 0.2844\n",
            "Epoch 395/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6036 - auc: 0.6639 - ap: 0.2709\n",
            "Epoch 396/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6040 - auc: 0.6572 - ap: 0.2755\n",
            "Epoch 397/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6039 - auc: 0.6668 - ap: 0.2714\n",
            "Epoch 398/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6036 - auc: 0.6712 - ap: 0.2898\n",
            "Epoch 399/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6042 - auc: 0.6584 - ap: 0.2661\n",
            "Epoch 400/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6038 - auc: 0.6686 - ap: 0.2809\n",
            "Epoch 401/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6036 - auc: 0.6594 - ap: 0.2711\n",
            "Epoch 402/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6038 - auc: 0.6792 - ap: 0.2914\n",
            "Epoch 403/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6036 - auc: 0.6574 - ap: 0.2762\n",
            "Epoch 404/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6037 - auc: 0.6610 - ap: 0.2758\n",
            "Epoch 405/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6036 - auc: 0.6537 - ap: 0.2631\n",
            "Epoch 406/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6035 - auc: 0.6699 - ap: 0.2875\n",
            "Epoch 407/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6044 - auc: 0.6553 - ap: 0.2649\n",
            "Epoch 408/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6040 - auc: 0.6591 - ap: 0.2771\n",
            "Epoch 409/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6038 - auc: 0.6629 - ap: 0.2940\n",
            "Epoch 410/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6032 - auc: 0.6578 - ap: 0.2936\n",
            "Epoch 411/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6039 - auc: 0.6604 - ap: 0.2750\n",
            "Epoch 412/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6036 - auc: 0.6662 - ap: 0.2987\n",
            "Epoch 413/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6030 - auc: 0.6623 - ap: 0.2702\n",
            "Epoch 414/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6038 - auc: 0.6569 - ap: 0.2689\n",
            "Epoch 415/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6035 - auc: 0.6747 - ap: 0.2847\n",
            "Epoch 416/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6032 - auc: 0.6411 - ap: 0.2611\n",
            "Epoch 417/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6037 - auc: 0.6679 - ap: 0.2658\n",
            "Epoch 418/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6034 - auc: 0.6555 - ap: 0.2626\n",
            "Epoch 419/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6036 - auc: 0.6669 - ap: 0.2904\n",
            "Epoch 420/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6034 - auc: 0.6589 - ap: 0.2666\n",
            "Epoch 421/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6032 - auc: 0.6650 - ap: 0.2760\n",
            "Epoch 422/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6039 - auc: 0.6597 - ap: 0.2824\n",
            "Epoch 423/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6031 - auc: 0.6633 - ap: 0.2721\n",
            "Epoch 424/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6031 - auc: 0.6723 - ap: 0.2805\n",
            "Epoch 425/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6034 - auc: 0.6548 - ap: 0.2775\n",
            "Epoch 426/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6030 - auc: 0.6710 - ap: 0.2745\n",
            "Epoch 427/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6031 - auc: 0.6632 - ap: 0.2695\n",
            "Epoch 428/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6029 - auc: 0.6635 - ap: 0.2863\n",
            "Epoch 429/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6031 - auc: 0.6535 - ap: 0.2767\n",
            "Epoch 430/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6031 - auc: 0.6579 - ap: 0.2714\n",
            "Epoch 431/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6029 - auc: 0.6560 - ap: 0.2858\n",
            "Epoch 432/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6035 - auc: 0.6650 - ap: 0.2733\n",
            "Epoch 433/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6031 - auc: 0.6499 - ap: 0.2626\n",
            "Epoch 434/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6030 - auc: 0.6655 - ap: 0.2752\n",
            "Epoch 435/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6032 - auc: 0.6579 - ap: 0.2692\n",
            "Epoch 436/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6035 - auc: 0.6675 - ap: 0.2971\n",
            "Epoch 437/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6032 - auc: 0.6509 - ap: 0.2684\n",
            "Epoch 438/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6035 - auc: 0.6687 - ap: 0.2870\n",
            "Epoch 439/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6033 - auc: 0.6587 - ap: 0.2750\n",
            "Epoch 440/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6034 - auc: 0.6696 - ap: 0.2881\n",
            "Epoch 441/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6032 - auc: 0.6518 - ap: 0.2807\n",
            "Epoch 442/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6034 - auc: 0.6713 - ap: 0.2808\n",
            "Epoch 443/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6034 - auc: 0.6525 - ap: 0.2688\n",
            "Epoch 444/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6036 - auc: 0.6693 - ap: 0.2826\n",
            "Epoch 445/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6034 - auc: 0.6461 - ap: 0.2737\n",
            "Epoch 446/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6031 - auc: 0.6581 - ap: 0.2754\n",
            "Epoch 447/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6031 - auc: 0.6516 - ap: 0.2673\n",
            "Epoch 448/500\n",
            "9/9 [==============================] - 0s 28ms/step - loss: 0.6033 - auc: 0.6663 - ap: 0.2885\n",
            "Epoch 449/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6031 - auc: 0.6518 - ap: 0.2679\n",
            "Epoch 450/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6032 - auc: 0.6622 - ap: 0.2835\n",
            "Epoch 451/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6033 - auc: 0.6686 - ap: 0.2693\n",
            "Epoch 452/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6031 - auc: 0.6656 - ap: 0.2696\n",
            "Epoch 453/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6033 - auc: 0.6660 - ap: 0.2781\n",
            "Epoch 454/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6032 - auc: 0.6606 - ap: 0.2760\n",
            "Epoch 455/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6033 - auc: 0.6650 - ap: 0.2786\n",
            "Epoch 456/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6034 - auc: 0.6606 - ap: 0.2674\n",
            "Epoch 457/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6030 - auc: 0.6583 - ap: 0.2664\n",
            "Epoch 458/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6035 - auc: 0.6720 - ap: 0.2728\n",
            "Epoch 459/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6036 - auc: 0.6601 - ap: 0.2656\n",
            "Epoch 460/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6036 - auc: 0.6682 - ap: 0.2822\n",
            "Epoch 461/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6032 - auc: 0.6459 - ap: 0.2545\n",
            "Epoch 462/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6034 - auc: 0.6694 - ap: 0.2785\n",
            "Epoch 463/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6036 - auc: 0.6579 - ap: 0.2762\n",
            "Epoch 464/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6036 - auc: 0.6736 - ap: 0.2868\n",
            "Epoch 465/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6039 - auc: 0.6536 - ap: 0.2727\n",
            "Epoch 466/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6035 - auc: 0.6677 - ap: 0.2765\n",
            "Epoch 467/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6036 - auc: 0.6577 - ap: 0.2756\n",
            "Epoch 468/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6043 - auc: 0.6547 - ap: 0.2752\n",
            "Epoch 469/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6037 - auc: 0.6494 - ap: 0.2704\n",
            "Epoch 470/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6037 - auc: 0.6557 - ap: 0.2892\n",
            "Epoch 471/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6036 - auc: 0.6448 - ap: 0.2517\n",
            "Epoch 472/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6035 - auc: 0.6615 - ap: 0.2638\n",
            "Epoch 473/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6037 - auc: 0.6668 - ap: 0.2843\n",
            "Epoch 474/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6038 - auc: 0.6610 - ap: 0.2660\n",
            "Epoch 475/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6034 - auc: 0.6663 - ap: 0.2596\n",
            "Epoch 476/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6039 - auc: 0.6687 - ap: 0.2761\n",
            "Epoch 477/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6034 - auc: 0.6528 - ap: 0.2700\n",
            "Epoch 478/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6032 - auc: 0.6541 - ap: 0.2616\n",
            "Epoch 479/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6034 - auc: 0.6502 - ap: 0.2642\n",
            "Epoch 480/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6038 - auc: 0.6662 - ap: 0.2755\n",
            "Epoch 481/500\n",
            "9/9 [==============================] - 0s 27ms/step - loss: 0.6039 - auc: 0.6534 - ap: 0.2609\n",
            "Epoch 482/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6036 - auc: 0.6540 - ap: 0.2767\n",
            "Epoch 483/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6033 - auc: 0.6495 - ap: 0.2624\n",
            "Epoch 484/500\n",
            "9/9 [==============================] - 0s 24ms/step - loss: 0.6029 - auc: 0.6699 - ap: 0.2867\n",
            "Epoch 485/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6038 - auc: 0.6493 - ap: 0.2722\n",
            "Epoch 486/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6031 - auc: 0.6666 - ap: 0.2778\n",
            "Epoch 487/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6031 - auc: 0.6441 - ap: 0.2743\n",
            "Epoch 488/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6032 - auc: 0.6764 - ap: 0.2799\n",
            "Epoch 489/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6029 - auc: 0.6550 - ap: 0.2800\n",
            "Epoch 490/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6030 - auc: 0.6663 - ap: 0.2961\n",
            "Epoch 491/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6026 - auc: 0.6624 - ap: 0.2812\n",
            "Epoch 492/500\n",
            "9/9 [==============================] - 0s 25ms/step - loss: 0.6032 - auc: 0.6631 - ap: 0.2853\n",
            "Epoch 493/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6028 - auc: 0.6525 - ap: 0.2790\n",
            "Epoch 494/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6027 - auc: 0.6633 - ap: 0.2709\n",
            "Epoch 495/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6030 - auc: 0.6608 - ap: 0.2675\n",
            "Epoch 496/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6025 - auc: 0.6651 - ap: 0.2837\n",
            "Epoch 497/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6030 - auc: 0.6702 - ap: 0.2859\n",
            "Epoch 498/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6029 - auc: 0.6697 - ap: 0.2862\n",
            "Epoch 499/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6027 - auc: 0.6631 - ap: 0.2836\n",
            "Epoch 500/500\n",
            "9/9 [==============================] - 0s 26ms/step - loss: 0.6029 - auc: 0.6742 - ap: 0.2660\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f653e97fbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf4kPTAxwTi7",
        "outputId": "b2f8388d-2447-4c6c-fd7d-2bbaf85a55a0"
      },
      "source": [
        "x_predict = model_rsrae.predict(x)[2]\n",
        "\n",
        "auc = roc_auc_score(y, cosine_similarity(x_predict, x))\n",
        "ap = average_precision_score(y, cosine_similarity(x_predict, x))\n",
        "                                    \n",
        "print(\"auc = \", auc)\n",
        "print(\"ap = \", ap)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "auc =  0.6618016644693472\n",
            "ap =  0.23899051205435262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjilMYq6yjuh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}