{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RSRAE_20newsgroups.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC7Kq3lHMNBd"
      },
      "source": [
        "## Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYmgkGW5LhVO",
        "outputId": "b62450fb-5ea4-48f7-cdfe-ed4a4d4b2294"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "\n",
        "c = 0.1  # отношение количества аномальных экземпляров к нормальным\n",
        "\n",
        "# загужаем данные\n",
        "normal_data = fetch_20newsgroups(subset='all', categories=['sci.electronics'],\n",
        "                               shuffle=True, random_state=123, \n",
        "                               remove=['headers', 'footers'], return_X_y=True)[0]\n",
        "anomal_data = fetch_20newsgroups(subset='all', categories=['talk.politics.mideast'],\n",
        "                               shuffle=True, random_state=123,\n",
        "                               remove=['headers', 'footers'],\n",
        "                               return_X_y=True)[0][:int(c * len(normal_data)) + 1]\n",
        "\n",
        "# # приводим к одинаковой длине\n",
        "# min_len = max(len(normal_data), len(anomal_data))\n",
        "# normal_data = normal_data[:min_len]\n",
        "# anomal_data = anomal_data[:min_len]\n",
        "print(\"Количество нормальных экземпляров = {}\".format(len(normal_data)))\n",
        "print(\"Количество аномальных экземпляров = {}\".format(len(anomal_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Количество нормальных экземпляров = 984\n",
            "Количество аномальных экземпляров = 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svfj5oa7MJQ7"
      },
      "source": [
        "## Предобработка текстовых данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orVDv2w4MCXH",
        "outputId": "6f311e77-afde-44c6-d79c-95ad379174da"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.parsing.preprocessing import strip_short\n",
        "from gensim.parsing.preprocessing import strip_non_alphanum\n",
        "from gensim.parsing.preprocessing import strip_numeric\n",
        "from gensim.utils import tokenize\n",
        "import nltk; nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    \"\"\"Удаление html tags из текста.\"\"\"\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text(separator=\" \")\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = strip_html_tags(text)  # удаление html tags\n",
        "    text = strip_non_alphanum(text) # заменили все небуквенные символы на пробел\n",
        "    text = strip_numeric(text) # удалили все цифры\n",
        "    text = remove_stopwords(text) # удалили все стоп-слова\n",
        "    text = strip_short(text, minsize=2) # удалили короткие слова\n",
        "    word_list = list(tokenize(text, deacc=True, to_lower=True)) # токенизация, deacc - избавляет от ударений\n",
        "    word_list = [WordNetLemmatizer().lemmatize(word) for word in word_list] # лемматизация\n",
        "    return ' '.join(word for word in word_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mPZjfpXMVJF"
      },
      "source": [
        "normal_data = [preprocess_text(text) for text in normal_data]\n",
        "anomal_data = [preprocess_text(text) for text in anomal_data]\n",
        "all_data = normal_data + anomal_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rar8q1p5MVnn",
        "outputId": "c54cafa6-da78-4608-b457-c00398240654"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "len_all_data = np.array([len(text.split(' ')) for text in all_data])\n",
        "# print(len(len_all_data[len_all_data <= 150]))\n",
        "print(\"mean length of sentence: \" + str(len_all_data.mean()))\n",
        "print(\"max length of sentence: \" + str(len_all_data.max()))\n",
        "print(\"std dev length of sentence: \" + str(len_all_data.std()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean length of sentence: 108.66943674976916\n",
            "max length of sentence: 6252\n",
            "std dev length of sentence: 273.15870543495356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOwAuraz2F1-"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alInBxb6PA6s"
      },
      "source": [
        "# from sklearn.feature_extraction.text import HashingVectorizer\n",
        "# from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# h_vectorizer = HashingVectorizer(ngram_range=(1, 1))\n",
        "# all_data_counts = h_vectorizer.fit_transform(all_data)\n",
        "# tf_transformer = TfidfTransformer(use_idf=True).fit(all_data_counts)\n",
        "# all_data_tf = tf_transformer.transform(all_data_counts).toarray()\n",
        "\n",
        "# print(all_data_tf.shape)\n",
        "# print(all_data_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EVOLW811LWn",
        "outputId": "f9333b63-cbb4-4a11-a124-20555bf2f549"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "all_data_tf = vectorizer.fit_transform(all_data).toarray()\n",
        "\n",
        "print(type(all_data_tf))\n",
        "print(all_data_tf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(1083, 13828)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsK3-rVC2f60"
      },
      "source": [
        "## Формирование выборок"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBdCv-uRM7v0",
        "outputId": "bef55faf-3a66-497b-c361-b99b85788bce"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "x = all_data_tf\n",
        "y = np.array([False] * len(normal_data) + [True] * len(anomal_data))\n",
        "\n",
        "all_data, x, y = shuffle(all_data, x, y, random_state=123)\n",
        "print(\"Всего экземпляров = {}\".format(len(all_data)))\n",
        "print(\"(Кол-во текстов, число признаков текста) = {}\".format(x.shape))\n",
        "print(\"Кол-во меток = {}\".format(len(y)))\n",
        "print(\"Кол-во нормальных экземпляров = {}\".format(len(normal_data)))\n",
        "print(\"Кол-во аномальных экземпляров = {}\".format(len(anomal_data)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Всего экземпляров = 1083\n",
            "(Кол-во текстов, число признаков текста) = (1083, 13828)\n",
            "Кол-во меток = 1083\n",
            "Кол-во нормальных экземпляров = 984\n",
            "Кол-во аномальных экземпляров = 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0MuCDL947Yx"
      },
      "source": [
        "## Функция Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXSYXbdS47GV"
      },
      "source": [
        "def cosine_similarity(x_predict, x):\n",
        "    if type(x_predict) is np.ndarray:\n",
        "        flat_output = np.reshape(x_predict, (np.shape(x)[0], -1))\n",
        "        flat_input = np.reshape(x_predict, (np.shape(x)[0], -1))\n",
        "        sum = np.sum(flat_output * flat_input, -1)\n",
        "        norm1 = np.linalg.norm(flat_output, axis=-1) + 0.000001\n",
        "        norm2 = np.linalg.norm(flat_input, axis=-1) + 0.000001 \n",
        "        return -(sum / norm1 / norm2)\n",
        "    else:\n",
        "        # ДЛЯ НЕ ПОЛНОСВЯЗНЫХ СЛОЕВ НУЖЕН ДРУГОЙ shape\n",
        "        flat_output = tf.reshape(tensor=x_predict, shape=[x.shape.as_list()[0], -1])\n",
        "        flat_input = tf.reshape(tensor=x_predict, shape=[x.shape.as_list()[0], -1])\n",
        "        sum = tf.math.reduce_sum(tf.math.multiply(flat_output, flat_input), axis=-1)\n",
        "        norm1 = tf.norm(flat_output, axis=-1) + 0.000001\n",
        "        norm2 = tf.norm(flat_input, axis=-1) + 0.000001\n",
        "        return -(tf.math.divide(tf.math.divide(sum, norm1), norm2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eVyaW7_5AW3"
      },
      "source": [
        "## RSRAE model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBVNmgjX4EpR",
        "outputId": "74b34454-a5ca-4c7e-c5b2-b43535750cdc"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version = {}\".format(tf.__version__)) # текущая версия tf\n",
        "\n",
        "from tensorflow.keras import Model, optimizers, metrics\n",
        "from tensorflow.keras.layers import Layer, Flatten, Dense, BatchNormalization\n",
        "\n",
        "# from tensorflow.keras import activations, Sequential, Input\n",
        "# from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Reshape\n",
        "# from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# Задаем random_seed для tensorflow и numpy\n",
        "random_seed = 123\n",
        "tf.random.set_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "# Sets the default float type\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "# Set random seed\n",
        "tf.random.set_seed(123)\n",
        "np.random.seed(123)\n",
        "\n",
        "\n",
        "class RSR(Layer):\n",
        "    \"\"\"\n",
        "    Robust Subspace Recovery (RSR) layer.\n",
        "    Робастный слой, восстанавливающий подпространство. Задача данного слоя - отобразить\n",
        "    закодированные энкодером данные в подпростраство так, чтобы после их обратного\n",
        "    отображения декодером дивергенция между экземпляром исходных данных и его образом,\n",
        "    полученным от автоэнкодера была незначительной для нормального экземпляра и была\n",
        "    большой для аномального экземпляра. \n",
        "\n",
        "    # Example\n",
        "    ```\n",
        "        z_rsr, A = RSR(intrinsic_size=10)(z)\n",
        "    ```\n",
        "    # Arguments\n",
        "        intrinsic_size: размерность z_rsr.\n",
        "    # Input shape\n",
        "        2D tensor with shape: `(n_samples, n_features)` after encoding.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(n_samples, intrinsic_size)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, intrinsic_size: int, name=\"RSR_layer\", **kwargs):\n",
        "        super(RSR, self).__init__(name=name, **kwargs)\n",
        "        # Если присваивать экземпляр слоя, как атрибут другого слоя, то хорошей\n",
        "        # практикой делать создавать такие подслои в __init__ (поскольку подслои обычно\n",
        "        # имеют метод build, они будут собраны, когда будет собран внешний слой). \n",
        "        self.flatten = Flatten()\n",
        "        self.intrinsic_size = intrinsic_size\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Определяет веса слоя, а именно задает матрицу A.\"\"\"\n",
        "        self.A = self.add_weight(name=\"A\",\n",
        "                                 shape=[int(input_shape[-1]), self.intrinsic_size],\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True,)\n",
        "        \n",
        "    def call(self, z):\n",
        "        \"\"\"\n",
        "        Логика слоя. Умножение выхода энкодера - вектора z на матрицу A.\n",
        "        Возвращает отображенный z_rsr и матрицу A, которая потребуется далее.\n",
        "        \"\"\"\n",
        "        z = self.flatten(z)\n",
        "        z_rsr = tf.linalg.matmul(z, self.A) \n",
        "        return z_rsr\n",
        "\n",
        "    # Опционально, пользовательский слой может быть сериализован реализацией метода \n",
        "    # get_config и метода класса (@classmethod) from_config.\n",
        "    def get_config(self):\n",
        "        config = super(Layer, self).get_config()\n",
        "        config.update({'intrinsic_size': self.intrinsic_size})\n",
        "        return config\n",
        "\n",
        "    # На самом деле нет необходимости определять `from_config` здесь, поскольку \n",
        "    # возвращение `cls(**config)` - поведение по умолчанию.\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "class L2Normalization(Layer):\n",
        "    \"\"\"Слой для l_2 нормализации, который будет применяться к выходу RSR layer.\"\"\"\n",
        "    \n",
        "    def __init__(self, name=\"L2Normalization\", **kwargs):\n",
        "        super(L2Normalization, self).__init__(name=name, **kwargs)\n",
        "\n",
        "    def call(self, z_rsr):\n",
        "        \"\"\"\n",
        "        Выполняет l_2 нормализацию векторов, полученных после применения RSR layer\n",
        "        вдоль оси, соответсвующей числу признаков. То есть производится нормализация\n",
        "        каждого экземпляра выборки, в результате которой признаки экземпляров будут\n",
        "        находиться в отрезке [-1; 1].\n",
        "        \"\"\"\n",
        "        z_tilde = tf.math.l2_normalize(z_rsr, axis=-1)\n",
        "        return z_tilde\n",
        "\n",
        "    # Опционально, пользовательский слой может быть сериализован реализацией метода \n",
        "    # get_config и метода класса (@classmethod) from_config.\n",
        "    def get_config(self):\n",
        "        config = super(Layer, self).get_config()\n",
        "        return config\n",
        "\n",
        "    # На самом деле нет необходимости определять `from_config` здесь, поскольку \n",
        "    # возвращение `cls(**config)` - поведение по умолчанию.\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "class Encoder(Layer):\n",
        "    \"\"\"\n",
        "    Класс для encoder модели RSRAE. Отображает исходные данные input_data в вектор z,\n",
        "    кодирующий исходные данные.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 hidden_layer_dimensions,\n",
        "                 activation,\n",
        "                 flag_bn=True, \n",
        "                 name=\"Encoder\",\n",
        "                 **kwargs):\n",
        "        super(Encoder, self).__init__(name=name, **kwargs)\n",
        "        self.hidden_layer_dimensions = hidden_layer_dimensions\n",
        "        self.activation = activation\n",
        "        self.flag_bn = flag_bn\n",
        "        self.dense0 = Dense(hidden_layer_dimensions[0], activation=activation,\n",
        "                            name='encoder_0')\n",
        "        self.dense1 = Dense(hidden_layer_dimensions[1], activation=activation,\n",
        "                            name='encoder_1')\n",
        "        self.dense2 = Dense(hidden_layer_dimensions[2], activation=activation,\n",
        "                            name='encoder_2')\n",
        "        if flag_bn:\n",
        "            self.batch_normalization0 = BatchNormalization(name=\"encoder_bn_layer_0\")\n",
        "            self.batch_normalization1 = BatchNormalization(name=\"encoder_bn_layer_1\")\n",
        "            self.batch_normalization2 = BatchNormalization(name=\"encoder_bn_layer_2\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Отображние исходных данных x -> в закодированный вектор z.\"\"\"\n",
        "        x = inputs\n",
        "        x = self.dense0(x)\n",
        "        if self.flag_bn:\n",
        "            x = self.batch_normalization0(x)\n",
        "        x = self.dense1(x)\n",
        "        if self.flag_bn:\n",
        "            x = self.batch_normalization1(x)\n",
        "        x = self.dense2(x)\n",
        "        if self.flag_bn:\n",
        "            x = self.batch_normalization2(x)\n",
        "        z = x\n",
        "        return z\n",
        "    \n",
        "    # Опционально, пользовательский слой может быть сериализован реализацией метода \n",
        "    # get_config и метода класса (@classmethod) from_config.\n",
        "    def get_config(self):\n",
        "        config = super(Layer, self).get_config()\n",
        "        config.update({'hidden_layer_dimensions': self.hidden_layer_dimensions})\n",
        "        config.update({'activation': self.activation})\n",
        "        config.update({'flag_bn': self.flag_bn})\n",
        "        return config\n",
        "\n",
        "    # На самом деле нет необходимости определять `from_config` здесь, поскольку \n",
        "    # возвращение `cls(**config)` - поведение по умолчанию.\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "class Decoder(Layer):\n",
        "    \"\"\"\n",
        "    Класс для decoder модели RSRAE. Отображает вектор z_rsr, полученный в результате\n",
        "    кодирования исходных данных в вектор z, и последующим отображением вектора z при\n",
        "    помощи RSR layer (x -> z -> z_rsr), обратно в пространство исходных данных \n",
        "    (z_rsr -> x_tilde).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 inputs_dim,\n",
        "                 hidden_layer_dimensions,\n",
        "                 activation,\n",
        "                 flag_bn=True, \n",
        "                 name=\"Decoder\",\n",
        "                 **kwargs):\n",
        "        super(Decoder, self).__init__(name=name, **kwargs)\n",
        "        self.hidden_layer_dimensions = hidden_layer_dimensions\n",
        "        self.activation = activation\n",
        "        self.flag_bn = flag_bn\n",
        "        self.dense2 = Dense(hidden_layer_dimensions[2], activation=activation,\n",
        "                            name='decoder_2')\n",
        "        self.dense1 = Dense(hidden_layer_dimensions[1], activation=activation,\n",
        "                            name='decoder_1')\n",
        "        self.dense0 = Dense(hidden_layer_dimensions[0], activation=activation,\n",
        "                            name='decoder_0')\n",
        "        self.dense_output = Dense(inputs_dim, activation=activation,\n",
        "                            name='decoder_output')\n",
        "        if flag_bn:\n",
        "            self.batch_normalization2 = BatchNormalization(name=\"decoder_bn_layer_2\")\n",
        "            self.batch_normalization1 = BatchNormalization(name=\"decoder_bn_layer_1\")\n",
        "            self.batch_normalization0 = BatchNormalization(name=\"decoder_bn_layer_0\")\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Отображние z_rsr -> x_tilde, где x_tilde - вектор, лежащий в пространстве\n",
        "        исходных даных.\n",
        "        \"\"\"\n",
        "        z_rsr = inputs\n",
        "        z_rsr = self.dense2(z_rsr)\n",
        "        if self.flag_bn:\n",
        "            z_rsr = self.batch_normalization2(z_rsr)\n",
        "        z_rsr = self.dense1(z_rsr)\n",
        "        if self.flag_bn:\n",
        "            z_rsr = self.batch_normalization1(z_rsr)\n",
        "        z_rsr = self.dense0(z_rsr)\n",
        "        if self.flag_bn:\n",
        "            z_rsr = self.batch_normalization0(z_rsr)\n",
        "        x_tilde = self.dense_output(z_rsr)\n",
        "        return x_tilde\n",
        "    \n",
        "    # Опционально, пользовательский слой может быть сериализован реализацией метода \n",
        "    # get_config и метода класса (@classmethod) from_config.\n",
        "    def get_config(self):\n",
        "        config = super(Layer, self).get_config()\n",
        "        config.update({'hidden_layer_dimensions': self.hidden_layer_dimensions})\n",
        "        config.update({'activation': self.activation})\n",
        "        config.update({'flag_bn': self.flag_bn})\n",
        "        return config\n",
        "\n",
        "    # На самом деле нет необходимости определять `from_config` здесь, поскольку \n",
        "    # возвращение `cls(**config)` - поведение по умолчанию.\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "class RSRAE(Model):\n",
        "    \"\"\"\n",
        "    Нейросетевая модель-автоэнкодер для обнаружения аномалий с робастным слоем,\n",
        "    восстанавливающим подпространство (RSR layer между encoder и decoder).\n",
        "    Комбинируем encoder + RSR layer + decoder в end-to-end модель.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 inputs_dim, # размерность вектора признаков\n",
        "                 hidden_layer_dimensions,\n",
        "                 intrinsic_size, # разерность z_rsr после RSR layer\n",
        "                 activation,\n",
        "                 flag_bn=True,\n",
        "                 flag_normalize=True,\n",
        "                 learning_rate=1e-3,\n",
        "                 ae_loss_norm_type='MSE',\n",
        "                 rsr_loss_norm_type='MSE',\n",
        "                 name='RSRAE',\n",
        "                 **kwargs):\n",
        "        super(RSRAE, self).__init__(name=name, **kwargs)\n",
        "        self.inputs_dim = inputs_dim\n",
        "        self.hidden_layer_dimensions = hidden_layer_dimensions\n",
        "        self.intrinsic_size = intrinsic_size\n",
        "        self.activation = activation\n",
        "        self.flag_bn = flag_bn\n",
        "        self.flag_normalize = flag_normalize\n",
        "        self.learning_rate = learning_rate\n",
        "        self.ae_loss_norm_type = ae_loss_norm_type\n",
        "        self.rsr_loss_norm_type = rsr_loss_norm_type\n",
        "        # Для вычисления среднего loss по loss всех батчей в эпохе\n",
        "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
        "        self.auc_tracker = metrics.Mean(name=\"auc\")\n",
        "        self.ap_tracker = metrics.Mean(name=\"ap\")\n",
        "\n",
        "        # Создание экземпляров оптимизаторов\n",
        "        self.optimizer_ae = optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.optimizer_rsr1 = optimizers.Adam(learning_rate=10 * learning_rate)\n",
        "        self.optimizer_rsr2 = optimizers.Adam(learning_rate=10 * learning_rate)\n",
        "\n",
        "        # Слои\n",
        "        self.encoder = Encoder(hidden_layer_dimensions=hidden_layer_dimensions,\n",
        "                               activation=activation,\n",
        "                               flag_bn=flag_bn)\n",
        "        self.rsr = RSR(intrinsic_size=intrinsic_size)\n",
        "        if flag_normalize:\n",
        "            self.l2normalization = L2Normalization()\n",
        "        self.decoder = Decoder(inputs_dim=inputs_dim,\n",
        "                               hidden_layer_dimensions=hidden_layer_dimensions,\n",
        "                               activation=activation,\n",
        "                               flag_bn=flag_bn)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        z = self.encoder(inputs)\n",
        "        z_rsr = self.rsr(z)\n",
        "        if self.flag_normalize:\n",
        "            z_rsr = self.l2normalization(z_rsr)\n",
        "        x_tilde = self.decoder(z_rsr)\n",
        "        return z, z_rsr, x_tilde\n",
        "\n",
        "    def ae_loss(self, x, x_tilde):\n",
        "        \"\"\"Функция потерь реконструкции автоэнкодера - L_AE.\"\"\"\n",
        "\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "        x_tilde = tf.reshape(x_tilde, (tf.shape(x_tilde)[0], -1))\n",
        "\n",
        "        # axis=1 для tf.norm => вычисление вдоль оси признаков\n",
        "        # tf.math.reduce_mean без параметров - mean от элементов матрицы\n",
        "        if self.ae_loss_norm_type in ['MSE', 'mse', 'Frob', 'F']:\n",
        "            return tf.math.reduce_mean(tf.math.square(tf.norm(x-x_tilde, \n",
        "                                                              ord=2, axis=1)))\n",
        "        elif self.ae_loss_norm_type in ['L1', 'l1']:\n",
        "            return tf.math.reduce_mean(tf.norm(x-x_tilde, ord=1, axis=1))\n",
        "        elif self.ae_loss_norm_type in ['LAD', 'lad', 'L21', 'l21', 'L2', 'l2']:\n",
        "            return tf.math.reduce_mean(tf.norm(x-x_tilde, ord=2, axis=1))\n",
        "        else:\n",
        "            raise Exception(\"Norm type error!\")\n",
        "    \n",
        "    def rsr1_loss(self, z, z_rsr):\n",
        "        \"\"\"Функция потери для RSR layer - L_RSR1.\"\"\"\n",
        "        z_rsr = tf.matmul(z_rsr, tf.transpose(self.rsr.A))\n",
        "\n",
        "        if self.rsr_loss_norm_type in ['MSE', 'mse', 'Frob', 'F']:\n",
        "            return tf.math.reduce_mean(tf.math.square(tf.norm(z-z_rsr, ord=2, \n",
        "                                                            axis=1)))\n",
        "        elif self.rsr_loss_norm_type in ['L1', 'l1']:\n",
        "            return tf.math.reduce_mean(tf.norm(z-z_rsr, ord=1, axis=1))\n",
        "        elif self.rsr_loss_norm_type in ['LAD', 'lad', 'L21', 'l21', 'L2', 'l2']:\n",
        "            return tf.math.reduce_mean(tf.norm(z-z_rsr, ord=2, axis=1))\n",
        "        else:\n",
        "            raise Exception(\"Norm type error!\")\n",
        "    \n",
        "    def rsr2_loss(self):\n",
        "        \"\"\"Функция потери для RSR layer - L_RSR2.\"\"\"\n",
        "        A = self.rsr.A\n",
        "        A_T = tf.transpose(A)\n",
        "        I = tf.eye(self.intrinsic_size, dtype=tf.float64)\n",
        "        return tf.math.reduce_mean(tf.math.square(tf.linalg.matmul(A_T, A) - I))\n",
        "        \n",
        "    def gradients(model, inputs, targets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss_value = loss_fn(model, inputs, targets)\n",
        "        return tape.gradient(loss_value, model.trainable_variables)\n",
        "    \n",
        "    @tf.function()\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Override the method. Будет вызываться при 'model.fit()'.\n",
        "        Один шаг обучения, на котором вычисляются функции потерь для автоэнкодера и\n",
        "        RSR layer, и в соотвествии с ними обновляются значения обучаемых переменных - \n",
        "        весов нейросети и матрицы A соответсвенно. Будет вызываться от одного батча.\n",
        "        Заметим, что в этом методе мы используем пользовательские оптимизаторы и функции\n",
        "        потерь, поэтому перед тренировкой метод compile вызывать не придется.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        x, y = data\n",
        "\n",
        "        # tf.GradientTape() - записывает операции для автоматического дифференцирования\n",
        "\n",
        "        # По умолчанию persistent=False и удерживаемые GradientTape, высвобождаются,\n",
        "        # как только вызывается метод GradientTape.gradient(). Чтобы вычислить несколько\n",
        "        # градиентов за одно вычисление, требуется задать persistent=true. Это позволяет\n",
        "        # многократно вызывать метод gradient(), тогда требуется самостоятельно\n",
        "        # освободить ресурсы с помощью 'del tape'.\n",
        "\n",
        "        # watch_accessed_variables=True => автоматическое отслеживание всех обучаемых\n",
        "        # переменные, к которым осуществляется доступ. Так градиенты могут быть\n",
        "        # запрошены c любого вычисленного результата в tape.\n",
        "        with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tape:\n",
        "            # Здесь требуется запустить прямой проход нейросети. Операции применяемые\n",
        "            # при проходе к входных данным будут записаны на GradientTape. \n",
        "            z, z_rsr, x_tilde = self.call(x) # прямой проход RSRAE\n",
        "            z = tf.keras.layers.Flatten()(z) # вроде для текстовых данных необязательно\n",
        "            # Вычисляем значения функций потерь для этого прохода\n",
        "            loss_ae = self.ae_loss(x, x_tilde)\n",
        "            loss_rsr1 = self.rsr1_loss(z, z_rsr)\n",
        "            loss_rsr2 = self.rsr2_loss()\n",
        "  \n",
        "        # Метод gradient вычисляет градиенты обучаемых параметров(весов) для минимизации\n",
        "        # функции потерь, используя операции, записанные в контексте этого tape.\n",
        "        gradients_ae = tape.gradient(loss_ae, self.trainable_weights)\n",
        "        gradients_rsr1 = tape.gradient(loss_rsr1, self.rsr.A)\n",
        "        gradients_rsr2 = tape.gradient(loss_rsr2, self.rsr.A)\n",
        "\n",
        "        # Обновим значения обучаемых переменных - градиентный шаг чтобы min loss.\n",
        "        self.optimizer_ae.apply_gradients(grads_and_vars=\n",
        "                                          zip(gradients_ae, self.trainable_weights))\n",
        "        self.optimizer_rsr1.apply_gradients(grads_and_vars=\n",
        "                                            zip([gradients_rsr1], [self.rsr.A]))\n",
        "        self.optimizer_rsr2.apply_gradients(grads_and_vars=\n",
        "                                            zip([gradients_rsr2], [self.rsr.A]))\n",
        "        \n",
        "        self.loss_tracker.update_state(loss_ae) # обновляем средний loss по батчам\n",
        "\n",
        "        # Обновляем метрики\n",
        "        if len(tf.unique(y)[0]) == 2:\n",
        "            # иначе roc_auc_score бросит ValueError и обучение приостановится\n",
        "            auc = self.auc_metric(y, cosine_similarity(x_tilde, x))\n",
        "            self.auc_tracker.update_state(auc)\n",
        "\n",
        "        ap = self.ap_metric(y, cosine_similarity(x_tilde, x))\n",
        "        self.ap_tracker.update_state(ap)\n",
        "\n",
        "        del tape # persistent=True => требуется самостоятельно освободить ресурсы\n",
        "        return {\"loss\": self.loss_tracker.result(),\n",
        "                \"auc\": self.auc_tracker.result(),\n",
        "                \"ap\": self.ap_tracker.result(),}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        \"\"\"\n",
        "        В пару к train_step. Сбрасывает метрики (`reset_states()`) в начале каждой\n",
        "        эпохи обучения с помощью 'fit()'. Без этого свойства 'result()' будет \n",
        "        возвращать среднее значение с начала обучения.\n",
        "        \"\"\"\n",
        "        return [self.loss_tracker, self.auc_tracker, self.ap_tracker]\n",
        "\n",
        "    def auc_metric(self, y_true, y_pred):\n",
        "        return tf.py_function(roc_auc_score, (y_true, y_pred), tf.float64)\n",
        "\n",
        "    def ap_metric(self, y_true, y_pred):\n",
        "        return tf.py_function(average_precision_score, (y_true, y_pred), tf.float64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version = 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zw6_gDz5InT"
      },
      "source": [
        "## Тренировка модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGDTtcOK5DSQ",
        "outputId": "37d5464b-f1d5-46b0-e97b-f1ec8e1df250"
      },
      "source": [
        "model_rsrae = RSRAE(inputs_dim=x.shape[-1],\n",
        "                    hidden_layer_dimensions=[64, 128, 256],\n",
        "                    intrinsic_size=20,\n",
        "                    activation='relu',\n",
        "                    learning_rate=1e-3,\n",
        "                    ae_loss_norm_type='MSE',\n",
        "                 rsr_loss_norm_type='MSE',)\n",
        "model_rsrae.compile(run_eagerly=True)\n",
        "model_rsrae.fit(x, y,\n",
        "                batch_size=128,\n",
        "                epochs=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "9/9 [==============================] - 2s 243ms/step - loss: 1.0093 - auc: 0.4861 - ap: 0.1332\n",
            "Epoch 2/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.9891 - auc: 0.4161 - ap: 0.1002\n",
            "Epoch 3/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.9851 - auc: 0.4451 - ap: 0.1058\n",
            "Epoch 4/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.9831 - auc: 0.5173 - ap: 0.1257\n",
            "Epoch 5/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.9818 - auc: 0.5709 - ap: 0.1477\n",
            "Epoch 6/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.9807 - auc: 0.6337 - ap: 0.1875\n",
            "Epoch 7/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.9788 - auc: 0.6660 - ap: 0.2052\n",
            "Epoch 8/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.9765 - auc: 0.7027 - ap: 0.2395\n",
            "Epoch 9/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.9730 - auc: 0.7026 - ap: 0.2279\n",
            "Epoch 10/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.9688 - auc: 0.6871 - ap: 0.2204\n",
            "Epoch 11/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.9633 - auc: 0.7075 - ap: 0.2590\n",
            "Epoch 12/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.9555 - auc: 0.6764 - ap: 0.2295\n",
            "Epoch 13/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.9470 - auc: 0.6267 - ap: 0.1756\n",
            "Epoch 14/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.9359 - auc: 0.5524 - ap: 0.1466\n",
            "Epoch 15/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.9241 - auc: 0.5475 - ap: 0.1624\n",
            "Epoch 16/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.9102 - auc: 0.5713 - ap: 0.1990\n",
            "Epoch 17/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.8953 - auc: 0.5728 - ap: 0.1768\n",
            "Epoch 18/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.8783 - auc: 0.5749 - ap: 0.1979\n",
            "Epoch 19/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.8622 - auc: 0.5595 - ap: 0.1646\n",
            "Epoch 20/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.8490 - auc: 0.5902 - ap: 0.1730\n",
            "Epoch 21/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.8303 - auc: 0.5984 - ap: 0.2100\n",
            "Epoch 22/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.8120 - auc: 0.6255 - ap: nan\n",
            "Epoch 23/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
            "  recall = tps / tps[-1]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 1s 154ms/step - loss: 0.7986 - auc: 0.5724 - ap: 0.1825\n",
            "Epoch 24/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.7839 - auc: 0.6304 - ap: 0.1873\n",
            "Epoch 25/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.7686 - auc: 0.6089 - ap: 0.1797\n",
            "Epoch 26/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.7560 - auc: 0.6288 - ap: 0.1827\n",
            "Epoch 27/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.7423 - auc: 0.6150 - ap: 0.1694\n",
            "Epoch 28/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.7297 - auc: 0.6542 - ap: 0.1953\n",
            "Epoch 29/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.7188 - auc: 0.6442 - ap: 0.1935\n",
            "Epoch 30/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.7068 - auc: 0.6564 - ap: 0.1720\n",
            "Epoch 31/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6980 - auc: 0.6477 - ap: 0.1796\n",
            "Epoch 32/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.6923 - auc: 0.6865 - ap: 0.1985\n",
            "Epoch 33/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.6839 - auc: 0.6580 - ap: 0.1793\n",
            "Epoch 34/500\n",
            "9/9 [==============================] - 1s 150ms/step - loss: 0.6784 - auc: 0.6631 - ap: 0.1831\n",
            "Epoch 35/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6720 - auc: 0.6658 - ap: 0.1795\n",
            "Epoch 36/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6654 - auc: 0.6588 - ap: 0.1651\n",
            "Epoch 37/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6598 - auc: 0.6862 - ap: 0.1877\n",
            "Epoch 38/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.6552 - auc: 0.6662 - ap: 0.1906\n",
            "Epoch 39/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6513 - auc: 0.6827 - ap: 0.1829\n",
            "Epoch 40/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6489 - auc: 0.6878 - ap: 0.1862\n",
            "Epoch 41/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6463 - auc: 0.6775 - ap: 0.1827\n",
            "Epoch 42/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.6411 - auc: 0.7018 - ap: 0.2055\n",
            "Epoch 43/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6383 - auc: 0.7127 - ap: 0.1964\n",
            "Epoch 44/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6390 - auc: 0.6720 - ap: 0.1854\n",
            "Epoch 45/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6322 - auc: 0.6936 - ap: 0.2080\n",
            "Epoch 46/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6314 - auc: 0.6999 - ap: 0.1908\n",
            "Epoch 47/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6285 - auc: 0.7073 - ap: 0.2171\n",
            "Epoch 48/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.6238 - auc: 0.6899 - ap: 0.1937\n",
            "Epoch 49/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.6241 - auc: 0.7152 - ap: 0.2222\n",
            "Epoch 50/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6220 - auc: 0.6971 - ap: 0.1924\n",
            "Epoch 51/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.6234 - auc: 0.6752 - ap: 0.1846\n",
            "Epoch 52/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6219 - auc: 0.7241 - ap: 0.2316\n",
            "Epoch 53/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.6223 - auc: 0.7051 - ap: 0.2112\n",
            "Epoch 54/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6218 - auc: 0.6944 - ap: 0.2045\n",
            "Epoch 55/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.6185 - auc: 0.7065 - ap: 0.2185\n",
            "Epoch 56/500\n",
            "9/9 [==============================] - 2s 194ms/step - loss: 0.6172 - auc: 0.7053 - ap: 0.2159\n",
            "Epoch 57/500\n",
            "9/9 [==============================] - 2s 254ms/step - loss: 0.6163 - auc: 0.7035 - ap: 0.2002\n",
            "Epoch 58/500\n",
            "9/9 [==============================] - 2s 247ms/step - loss: 0.6137 - auc: 0.7113 - ap: 0.2050\n",
            "Epoch 59/500\n",
            "9/9 [==============================] - 2s 250ms/step - loss: 0.6150 - auc: 0.6819 - ap: 0.1846\n",
            "Epoch 60/500\n",
            "9/9 [==============================] - 3s 322ms/step - loss: 0.6159 - auc: 0.6901 - ap: 0.2291\n",
            "Epoch 61/500\n",
            "9/9 [==============================] - 3s 308ms/step - loss: 0.6149 - auc: 0.7207 - ap: 0.2175\n",
            "Epoch 62/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6157 - auc: 0.6919 - ap: 0.2083\n",
            "Epoch 63/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6131 - auc: 0.7147 - ap: 0.2316\n",
            "Epoch 64/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.6129 - auc: 0.7123 - ap: 0.2112\n",
            "Epoch 65/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6111 - auc: 0.7192 - ap: 0.2149\n",
            "Epoch 66/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6071 - auc: 0.6986 - ap: 0.2093\n",
            "Epoch 67/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6092 - auc: 0.6940 - ap: 0.2134\n",
            "Epoch 68/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.6109 - auc: 0.7034 - ap: 0.1857\n",
            "Epoch 69/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6077 - auc: 0.7156 - ap: 0.2300\n",
            "Epoch 70/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6094 - auc: 0.7011 - ap: 0.2124\n",
            "Epoch 71/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6062 - auc: 0.7146 - ap: 0.2089\n",
            "Epoch 72/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6112 - auc: 0.6856 - ap: 0.2007\n",
            "Epoch 73/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6082 - auc: 0.7138 - ap: 0.2276\n",
            "Epoch 74/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6104 - auc: 0.7019 - ap: 0.1986\n",
            "Epoch 75/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.6088 - auc: 0.7005 - ap: 0.2149\n",
            "Epoch 76/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6087 - auc: 0.6948 - ap: 0.1931\n",
            "Epoch 77/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6064 - auc: 0.7125 - ap: 0.2198\n",
            "Epoch 78/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6066 - auc: 0.7072 - ap: 0.2215\n",
            "Epoch 79/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6054 - auc: 0.7017 - ap: 0.2036\n",
            "Epoch 80/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6038 - auc: 0.7069 - ap: 0.1927\n",
            "Epoch 81/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6065 - auc: 0.7219 - ap: 0.2380\n",
            "Epoch 82/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6056 - auc: 0.7120 - ap: 0.2175\n",
            "Epoch 83/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6055 - auc: 0.7290 - ap: 0.2243\n",
            "Epoch 84/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6045 - auc: 0.7036 - ap: 0.2150\n",
            "Epoch 85/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6070 - auc: 0.7075 - ap: 0.2103\n",
            "Epoch 86/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6034 - auc: 0.7112 - ap: 0.2054\n",
            "Epoch 87/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.6031 - auc: 0.7125 - ap: 0.2210\n",
            "Epoch 88/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6048 - auc: 0.6993 - ap: 0.1890\n",
            "Epoch 89/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.6052 - auc: 0.7197 - ap: 0.2137\n",
            "Epoch 90/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6043 - auc: 0.6972 - ap: 0.2087\n",
            "Epoch 91/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.6005 - auc: 0.6857 - ap: 0.2030\n",
            "Epoch 92/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.6045 - auc: 0.7380 - ap: 0.2194\n",
            "Epoch 93/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6019 - auc: 0.7061 - ap: 0.1940\n",
            "Epoch 94/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.6030 - auc: 0.6975 - ap: 0.1989\n",
            "Epoch 95/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6009 - auc: 0.7126 - ap: 0.2219\n",
            "Epoch 96/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.6022 - auc: 0.7092 - ap: 0.2080\n",
            "Epoch 97/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.6044 - auc: 0.7209 - ap: 0.2011\n",
            "Epoch 98/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6041 - auc: 0.7108 - ap: 0.2069\n",
            "Epoch 99/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.6051 - auc: 0.7176 - ap: 0.2162\n",
            "Epoch 100/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6020 - auc: 0.7217 - ap: 0.2047\n",
            "Epoch 101/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.5992 - auc: 0.7077 - ap: 0.1906\n",
            "Epoch 102/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6025 - auc: 0.7117 - ap: 0.2298\n",
            "Epoch 103/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6009 - auc: 0.6994 - ap: 0.1949\n",
            "Epoch 104/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6032 - auc: 0.7211 - ap: 0.2127\n",
            "Epoch 105/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6001 - auc: 0.7096 - ap: 0.2076\n",
            "Epoch 106/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6007 - auc: 0.7276 - ap: 0.2500\n",
            "Epoch 107/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6014 - auc: 0.7080 - ap: 0.2047\n",
            "Epoch 108/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6001 - auc: 0.7296 - ap: 0.2300\n",
            "Epoch 109/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.6003 - auc: 0.7001 - ap: 0.1955\n",
            "Epoch 110/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.6019 - auc: 0.7393 - ap: 0.2385\n",
            "Epoch 111/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.6000 - auc: 0.7031 - ap: 0.1929\n",
            "Epoch 112/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6019 - auc: 0.7188 - ap: 0.2543\n",
            "Epoch 113/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6026 - auc: 0.6966 - ap: 0.2036\n",
            "Epoch 114/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5988 - auc: 0.7382 - ap: 0.2592\n",
            "Epoch 115/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6009 - auc: 0.7054 - ap: 0.2231\n",
            "Epoch 116/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5971 - auc: 0.7278 - ap: 0.2298\n",
            "Epoch 117/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.6004 - auc: 0.7032 - ap: 0.2094\n",
            "Epoch 118/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5984 - auc: 0.7227 - ap: 0.2260\n",
            "Epoch 119/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5976 - auc: 0.7197 - ap: 0.2081\n",
            "Epoch 120/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6016 - auc: 0.7154 - ap: 0.2221\n",
            "Epoch 121/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6006 - auc: 0.7026 - ap: 0.2057\n",
            "Epoch 122/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.6004 - auc: 0.7284 - ap: 0.2136\n",
            "Epoch 123/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.5999 - auc: 0.7001 - ap: 0.2082\n",
            "Epoch 124/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5976 - auc: 0.7212 - ap: 0.2194\n",
            "Epoch 125/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.6000 - auc: 0.7248 - ap: 0.2202\n",
            "Epoch 126/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5994 - auc: 0.7088 - ap: 0.2056\n",
            "Epoch 127/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.6008 - auc: 0.7154 - ap: 0.2210\n",
            "Epoch 128/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.6007 - auc: 0.6952 - ap: 0.1817\n",
            "Epoch 129/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5966 - auc: 0.7308 - ap: 0.2289\n",
            "Epoch 130/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5994 - auc: 0.6970 - ap: 0.2063\n",
            "Epoch 131/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5990 - auc: 0.7068 - ap: 0.2182\n",
            "Epoch 132/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5984 - auc: 0.7023 - ap: 0.2024\n",
            "Epoch 133/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.6028 - auc: 0.7122 - ap: 0.1982\n",
            "Epoch 134/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5979 - auc: 0.7311 - ap: 0.2323\n",
            "Epoch 135/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5988 - auc: 0.6927 - ap: 0.2203\n",
            "Epoch 136/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5992 - auc: 0.7152 - ap: 0.2205\n",
            "Epoch 137/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.6006 - auc: 0.7252 - ap: 0.2281\n",
            "Epoch 138/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5988 - auc: 0.6911 - ap: 0.2163\n",
            "Epoch 139/500\n",
            "9/9 [==============================] - 2s 222ms/step - loss: 0.5991 - auc: 0.7282 - ap: 0.2139\n",
            "Epoch 140/500\n",
            "9/9 [==============================] - 2s 187ms/step - loss: 0.5978 - auc: 0.7236 - ap: 0.2312\n",
            "Epoch 141/500\n",
            "9/9 [==============================] - 2s 170ms/step - loss: 0.5988 - auc: 0.7163 - ap: 0.2115\n",
            "Epoch 142/500\n",
            "9/9 [==============================] - 2s 172ms/step - loss: 0.5994 - auc: 0.7243 - ap: 0.2035\n",
            "Epoch 143/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5974 - auc: 0.7220 - ap: 0.2258\n",
            "Epoch 144/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5985 - auc: 0.7231 - ap: 0.2256\n",
            "Epoch 145/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5981 - auc: 0.7244 - ap: 0.2217\n",
            "Epoch 146/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.5987 - auc: 0.7189 - ap: 0.2068\n",
            "Epoch 147/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5970 - auc: 0.7141 - ap: 0.2208\n",
            "Epoch 148/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6000 - auc: 0.7123 - ap: 0.2079\n",
            "Epoch 149/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.6012 - auc: 0.7212 - ap: 0.2319\n",
            "Epoch 150/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.6001 - auc: 0.7177 - ap: 0.2232\n",
            "Epoch 151/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5980 - auc: 0.7118 - ap: 0.2265\n",
            "Epoch 152/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5968 - auc: 0.7482 - ap: 0.2493\n",
            "Epoch 153/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5989 - auc: 0.6872 - ap: 0.2070\n",
            "Epoch 154/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5984 - auc: 0.6714 - ap: 0.2064\n",
            "Epoch 155/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5983 - auc: 0.7213 - ap: 0.2308\n",
            "Epoch 156/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5984 - auc: 0.7178 - ap: 0.2126\n",
            "Epoch 157/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5994 - auc: 0.7131 - ap: 0.2264\n",
            "Epoch 158/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5996 - auc: 0.7111 - ap: 0.2210\n",
            "Epoch 159/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.6004 - auc: 0.7433 - ap: 0.2190\n",
            "Epoch 160/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5990 - auc: 0.7079 - ap: 0.2216\n",
            "Epoch 161/500\n",
            "9/9 [==============================] - 1s 161ms/step - loss: 0.5965 - auc: 0.7166 - ap: 0.2273\n",
            "Epoch 162/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5989 - auc: 0.7205 - ap: 0.2244\n",
            "Epoch 163/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5969 - auc: 0.7140 - ap: 0.2069\n",
            "Epoch 164/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5958 - auc: 0.7214 - ap: 0.2272\n",
            "Epoch 165/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5975 - auc: 0.7343 - ap: 0.2508\n",
            "Epoch 166/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5981 - auc: 0.7270 - ap: 0.2341\n",
            "Epoch 167/500\n",
            "9/9 [==============================] - 1s 162ms/step - loss: 0.5986 - auc: 0.7238 - ap: 0.2047\n",
            "Epoch 168/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.5990 - auc: 0.7156 - ap: 0.2106\n",
            "Epoch 169/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5997 - auc: 0.7071 - ap: 0.2072\n",
            "Epoch 170/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5994 - auc: 0.7184 - ap: 0.2297\n",
            "Epoch 171/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5968 - auc: 0.7066 - ap: 0.2095\n",
            "Epoch 172/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5973 - auc: 0.7173 - ap: 0.2089\n",
            "Epoch 173/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5969 - auc: 0.7350 - ap: 0.2362\n",
            "Epoch 174/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5999 - auc: 0.7077 - ap: 0.2029\n",
            "Epoch 175/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5948 - auc: 0.7177 - ap: 0.2143\n",
            "Epoch 176/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5988 - auc: 0.7271 - ap: 0.2068\n",
            "Epoch 177/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5970 - auc: 0.7277 - ap: 0.2354\n",
            "Epoch 178/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5989 - auc: 0.7242 - ap: 0.2051\n",
            "Epoch 179/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.5963 - auc: 0.7176 - ap: 0.2069\n",
            "Epoch 180/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5941 - auc: 0.7199 - ap: 0.2331\n",
            "Epoch 181/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.5960 - auc: 0.7275 - ap: 0.2376\n",
            "Epoch 182/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5968 - auc: 0.7318 - ap: 0.2305\n",
            "Epoch 183/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5959 - auc: 0.7238 - ap: 0.2045\n",
            "Epoch 184/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5956 - auc: 0.7302 - ap: 0.2163\n",
            "Epoch 185/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5952 - auc: 0.7147 - ap: 0.2046\n",
            "Epoch 186/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5972 - auc: 0.7181 - ap: 0.2203\n",
            "Epoch 187/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5975 - auc: 0.7192 - ap: 0.2034\n",
            "Epoch 188/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5986 - auc: 0.7273 - ap: 0.2310\n",
            "Epoch 189/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5985 - auc: 0.7130 - ap: 0.2009\n",
            "Epoch 190/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5958 - auc: 0.7146 - ap: 0.1975\n",
            "Epoch 191/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5953 - auc: 0.6989 - ap: 0.2043\n",
            "Epoch 192/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5965 - auc: 0.7325 - ap: 0.2421\n",
            "Epoch 193/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5966 - auc: 0.7111 - ap: 0.2127\n",
            "Epoch 194/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5970 - auc: 0.7145 - ap: 0.2157\n",
            "Epoch 195/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5966 - auc: 0.7245 - ap: 0.2291\n",
            "Epoch 196/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5985 - auc: 0.7084 - ap: 0.2337\n",
            "Epoch 197/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5964 - auc: 0.7250 - ap: 0.2218\n",
            "Epoch 198/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5987 - auc: 0.7160 - ap: 0.2140\n",
            "Epoch 199/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5972 - auc: 0.7193 - ap: 0.2228\n",
            "Epoch 200/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5976 - auc: 0.7049 - ap: 0.2004\n",
            "Epoch 201/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5955 - auc: 0.7088 - ap: 0.2103\n",
            "Epoch 202/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5950 - auc: 0.7028 - ap: 0.1964\n",
            "Epoch 203/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5964 - auc: 0.7122 - ap: 0.2364\n",
            "Epoch 204/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5971 - auc: 0.7196 - ap: 0.2163\n",
            "Epoch 205/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.5971 - auc: 0.7144 - ap: 0.2388\n",
            "Epoch 206/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5968 - auc: 0.7208 - ap: 0.1911\n",
            "Epoch 207/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5971 - auc: 0.7354 - ap: 0.2354\n",
            "Epoch 208/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5958 - auc: 0.7119 - ap: 0.2141\n",
            "Epoch 209/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5972 - auc: 0.7440 - ap: 0.2383\n",
            "Epoch 210/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5971 - auc: 0.7069 - ap: 0.2036\n",
            "Epoch 211/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5957 - auc: 0.7169 - ap: 0.2016\n",
            "Epoch 212/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5970 - auc: 0.6998 - ap: 0.1962\n",
            "Epoch 213/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5983 - auc: 0.7126 - ap: 0.2133\n",
            "Epoch 214/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5963 - auc: 0.7278 - ap: 0.2221\n",
            "Epoch 215/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5976 - auc: 0.7135 - ap: 0.2098\n",
            "Epoch 216/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5974 - auc: 0.7040 - ap: 0.2120\n",
            "Epoch 217/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5955 - auc: 0.7268 - ap: 0.2336\n",
            "Epoch 218/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5964 - auc: 0.7204 - ap: 0.2068\n",
            "Epoch 219/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5982 - auc: 0.7213 - ap: 0.2268\n",
            "Epoch 220/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5997 - auc: 0.7385 - ap: 0.2198\n",
            "Epoch 221/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.5942 - auc: 0.7011 - ap: 0.2224\n",
            "Epoch 222/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5978 - auc: 0.7186 - ap: 0.2202\n",
            "Epoch 223/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5960 - auc: 0.7142 - ap: 0.2183\n",
            "Epoch 224/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5954 - auc: 0.7055 - ap: 0.2133\n",
            "Epoch 225/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5946 - auc: 0.7432 - ap: 0.2276\n",
            "Epoch 226/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5958 - auc: 0.7070 - ap: 0.2002\n",
            "Epoch 227/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5963 - auc: 0.7315 - ap: 0.2390\n",
            "Epoch 228/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5965 - auc: 0.7098 - ap: 0.2011\n",
            "Epoch 229/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5969 - auc: 0.7257 - ap: 0.2293\n",
            "Epoch 230/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5962 - auc: 0.6891 - ap: 0.2047\n",
            "Epoch 231/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5955 - auc: 0.7109 - ap: 0.2200\n",
            "Epoch 232/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5954 - auc: 0.7193 - ap: 0.2192\n",
            "Epoch 233/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5971 - auc: 0.7143 - ap: 0.2002\n",
            "Epoch 234/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5962 - auc: 0.7111 - ap: 0.2282\n",
            "Epoch 235/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5970 - auc: 0.7183 - ap: 0.2323\n",
            "Epoch 236/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5972 - auc: 0.7109 - ap: 0.2121\n",
            "Epoch 237/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5963 - auc: 0.7186 - ap: 0.2164\n",
            "Epoch 238/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5939 - auc: 0.7133 - ap: 0.2398\n",
            "Epoch 239/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5982 - auc: 0.7022 - ap: 0.2181\n",
            "Epoch 240/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5962 - auc: 0.6928 - ap: 0.2018\n",
            "Epoch 241/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5951 - auc: 0.7242 - ap: 0.2105\n",
            "Epoch 242/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5960 - auc: 0.7150 - ap: 0.2100\n",
            "Epoch 243/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5951 - auc: 0.7220 - ap: 0.2167\n",
            "Epoch 244/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5955 - auc: 0.7134 - ap: 0.2064\n",
            "Epoch 245/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5979 - auc: 0.7098 - ap: 0.2141\n",
            "Epoch 246/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5964 - auc: 0.7220 - ap: 0.2262\n",
            "Epoch 247/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5981 - auc: 0.7307 - ap: 0.2291\n",
            "Epoch 248/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5976 - auc: 0.6923 - ap: 0.2209\n",
            "Epoch 249/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5949 - auc: 0.7133 - ap: 0.2159\n",
            "Epoch 250/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5945 - auc: 0.7245 - ap: 0.2216\n",
            "Epoch 251/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5960 - auc: 0.7048 - ap: 0.1952\n",
            "Epoch 252/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5967 - auc: 0.6951 - ap: 0.2020\n",
            "Epoch 253/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5969 - auc: 0.7122 - ap: 0.2080\n",
            "Epoch 254/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5956 - auc: 0.7100 - ap: 0.1926\n",
            "Epoch 255/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5961 - auc: 0.7174 - ap: 0.2209\n",
            "Epoch 256/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5962 - auc: 0.7072 - ap: 0.2115\n",
            "Epoch 257/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5986 - auc: 0.7191 - ap: 0.2170\n",
            "Epoch 258/500\n",
            "9/9 [==============================] - 1s 151ms/step - loss: 0.5963 - auc: 0.7150 - ap: 0.1887\n",
            "Epoch 259/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5939 - auc: 0.7075 - ap: 0.2164\n",
            "Epoch 260/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5948 - auc: 0.7082 - ap: 0.1911\n",
            "Epoch 261/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5953 - auc: 0.6982 - ap: 0.1980\n",
            "Epoch 262/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5929 - auc: 0.7302 - ap: 0.2261\n",
            "Epoch 263/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5964 - auc: 0.7231 - ap: 0.2163\n",
            "Epoch 264/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5950 - auc: 0.7118 - ap: 0.2198\n",
            "Epoch 265/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5944 - auc: 0.7182 - ap: 0.2224\n",
            "Epoch 266/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5973 - auc: 0.7063 - ap: 0.2282\n",
            "Epoch 267/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5967 - auc: 0.7328 - ap: 0.2148\n",
            "Epoch 268/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5974 - auc: 0.7272 - ap: 0.2360\n",
            "Epoch 269/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5948 - auc: 0.7236 - ap: 0.2077\n",
            "Epoch 270/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5969 - auc: 0.7195 - ap: 0.2059\n",
            "Epoch 271/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5954 - auc: 0.7237 - ap: 0.2027\n",
            "Epoch 272/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5938 - auc: 0.7046 - ap: 0.1910\n",
            "Epoch 273/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5971 - auc: 0.7127 - ap: 0.2164\n",
            "Epoch 274/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5973 - auc: 0.7057 - ap: 0.2177\n",
            "Epoch 275/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5965 - auc: 0.7195 - ap: 0.2097\n",
            "Epoch 276/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5943 - auc: 0.7248 - ap: 0.2337\n",
            "Epoch 277/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5944 - auc: 0.7011 - ap: 0.2150\n",
            "Epoch 278/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5965 - auc: 0.7141 - ap: 0.2169\n",
            "Epoch 279/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5947 - auc: 0.7112 - ap: 0.2059\n",
            "Epoch 280/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5967 - auc: 0.7142 - ap: 0.2216\n",
            "Epoch 281/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5940 - auc: 0.7122 - ap: 0.2108\n",
            "Epoch 282/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5962 - auc: 0.7256 - ap: 0.2164\n",
            "Epoch 283/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5965 - auc: 0.7156 - ap: 0.1992\n",
            "Epoch 284/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5981 - auc: 0.7220 - ap: 0.2254\n",
            "Epoch 285/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5945 - auc: 0.7198 - ap: 0.2035\n",
            "Epoch 286/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5953 - auc: 0.7180 - ap: 0.2295\n",
            "Epoch 287/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5969 - auc: 0.6914 - ap: 0.1942\n",
            "Epoch 288/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5961 - auc: 0.7101 - ap: 0.2064\n",
            "Epoch 289/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5965 - auc: 0.7243 - ap: 0.2408\n",
            "Epoch 290/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5952 - auc: 0.7354 - ap: 0.2627\n",
            "Epoch 291/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5968 - auc: 0.7144 - ap: 0.2376\n",
            "Epoch 292/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5962 - auc: 0.7367 - ap: 0.2161\n",
            "Epoch 293/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5955 - auc: 0.7235 - ap: 0.2347\n",
            "Epoch 294/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5938 - auc: 0.7163 - ap: 0.2037\n",
            "Epoch 295/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5956 - auc: 0.7303 - ap: 0.2449\n",
            "Epoch 296/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5952 - auc: 0.7028 - ap: 0.2083\n",
            "Epoch 297/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5952 - auc: 0.7334 - ap: 0.2328\n",
            "Epoch 298/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5974 - auc: 0.6965 - ap: 0.2141\n",
            "Epoch 299/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5947 - auc: 0.7226 - ap: 0.2137\n",
            "Epoch 300/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5970 - auc: 0.7189 - ap: 0.2121\n",
            "Epoch 301/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5953 - auc: 0.7144 - ap: 0.2127\n",
            "Epoch 302/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5949 - auc: 0.7038 - ap: 0.2289\n",
            "Epoch 303/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5970 - auc: 0.7229 - ap: 0.2264\n",
            "Epoch 304/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5958 - auc: 0.7076 - ap: 0.1848\n",
            "Epoch 305/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5966 - auc: 0.7104 - ap: 0.2136\n",
            "Epoch 306/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5951 - auc: 0.7021 - ap: 0.2093\n",
            "Epoch 307/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5972 - auc: 0.7014 - ap: 0.1956\n",
            "Epoch 308/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.5960 - auc: 0.7001 - ap: 0.2136\n",
            "Epoch 309/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5949 - auc: 0.7158 - ap: 0.2207\n",
            "Epoch 310/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5943 - auc: 0.7231 - ap: 0.2361\n",
            "Epoch 311/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5961 - auc: 0.7243 - ap: 0.2282\n",
            "Epoch 312/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5942 - auc: 0.7463 - ap: 0.2409\n",
            "Epoch 313/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5931 - auc: 0.7168 - ap: 0.2173\n",
            "Epoch 314/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5971 - auc: 0.6983 - ap: 0.2131\n",
            "Epoch 315/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5956 - auc: 0.7111 - ap: 0.2262\n",
            "Epoch 316/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5957 - auc: 0.7099 - ap: 0.2037\n",
            "Epoch 317/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5948 - auc: 0.7205 - ap: 0.2450\n",
            "Epoch 318/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5959 - auc: 0.7145 - ap: 0.2269\n",
            "Epoch 319/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5972 - auc: 0.7249 - ap: 0.2218\n",
            "Epoch 320/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5945 - auc: 0.7168 - ap: 0.2109\n",
            "Epoch 321/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5953 - auc: 0.6996 - ap: 0.1964\n",
            "Epoch 322/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5960 - auc: 0.7121 - ap: 0.2245\n",
            "Epoch 323/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5943 - auc: 0.7085 - ap: 0.1963\n",
            "Epoch 324/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5952 - auc: 0.7158 - ap: 0.2110\n",
            "Epoch 325/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5961 - auc: 0.7111 - ap: 0.2172\n",
            "Epoch 326/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5959 - auc: 0.7146 - ap: 0.2156\n",
            "Epoch 327/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5959 - auc: 0.7117 - ap: 0.2179\n",
            "Epoch 328/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5956 - auc: 0.7039 - ap: 0.2305\n",
            "Epoch 329/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5948 - auc: 0.7234 - ap: 0.2417\n",
            "Epoch 330/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5966 - auc: 0.7215 - ap: 0.2322\n",
            "Epoch 331/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5956 - auc: 0.7190 - ap: 0.2151\n",
            "Epoch 332/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5945 - auc: 0.7072 - ap: 0.2148\n",
            "Epoch 333/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5944 - auc: 0.7077 - ap: 0.2193\n",
            "Epoch 334/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5966 - auc: 0.7153 - ap: 0.2189\n",
            "Epoch 335/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5942 - auc: 0.7184 - ap: 0.2227\n",
            "Epoch 336/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5961 - auc: 0.7222 - ap: 0.2171\n",
            "Epoch 337/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.5948 - auc: 0.7171 - ap: 0.2254\n",
            "Epoch 338/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5932 - auc: 0.7146 - ap: 0.2098\n",
            "Epoch 339/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5947 - auc: 0.7198 - ap: 0.2223\n",
            "Epoch 340/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5953 - auc: 0.7415 - ap: 0.2285\n",
            "Epoch 341/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5926 - auc: 0.7169 - ap: 0.2398\n",
            "Epoch 342/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5942 - auc: 0.7245 - ap: 0.2319\n",
            "Epoch 343/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5960 - auc: 0.7205 - ap: 0.2264\n",
            "Epoch 344/500\n",
            "9/9 [==============================] - 1s 152ms/step - loss: 0.5941 - auc: 0.7128 - ap: 0.2108\n",
            "Epoch 345/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5949 - auc: 0.7127 - ap: 0.1939\n",
            "Epoch 346/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5939 - auc: 0.7213 - ap: 0.2192\n",
            "Epoch 347/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5957 - auc: 0.7256 - ap: 0.2362\n",
            "Epoch 348/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5941 - auc: 0.7051 - ap: 0.2049\n",
            "Epoch 349/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5925 - auc: 0.7087 - ap: 0.2003\n",
            "Epoch 350/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5964 - auc: 0.7204 - ap: 0.2175\n",
            "Epoch 351/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5942 - auc: 0.7126 - ap: 0.2405\n",
            "Epoch 352/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5943 - auc: 0.7143 - ap: 0.2200\n",
            "Epoch 353/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5972 - auc: 0.7212 - ap: 0.2186\n",
            "Epoch 354/500\n",
            "9/9 [==============================] - 1s 161ms/step - loss: 0.5949 - auc: 0.7254 - ap: 0.2178\n",
            "Epoch 355/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5957 - auc: 0.7063 - ap: 0.2320\n",
            "Epoch 356/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5934 - auc: 0.7169 - ap: 0.2401\n",
            "Epoch 357/500\n",
            "9/9 [==============================] - 1s 162ms/step - loss: 0.5938 - auc: 0.7214 - ap: 0.2045\n",
            "Epoch 358/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5948 - auc: 0.7111 - ap: 0.2082\n",
            "Epoch 359/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5947 - auc: 0.7165 - ap: 0.2262\n",
            "Epoch 360/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5949 - auc: 0.7132 - ap: 0.2045\n",
            "Epoch 361/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5946 - auc: 0.7251 - ap: 0.2142\n",
            "Epoch 362/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5933 - auc: 0.7196 - ap: 0.2092\n",
            "Epoch 363/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5955 - auc: 0.7252 - ap: 0.2119\n",
            "Epoch 364/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5939 - auc: 0.7059 - ap: 0.2228\n",
            "Epoch 365/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5953 - auc: 0.7489 - ap: 0.2612\n",
            "Epoch 366/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5965 - auc: 0.7006 - ap: 0.2091\n",
            "Epoch 367/500\n",
            "9/9 [==============================] - 1s 160ms/step - loss: 0.5959 - auc: 0.7240 - ap: 0.2149\n",
            "Epoch 368/500\n",
            "9/9 [==============================] - 1s 160ms/step - loss: 0.5942 - auc: 0.7085 - ap: 0.2123\n",
            "Epoch 369/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5927 - auc: 0.7373 - ap: 0.2597\n",
            "Epoch 370/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5978 - auc: 0.7143 - ap: 0.2206\n",
            "Epoch 371/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5936 - auc: 0.7223 - ap: 0.2154\n",
            "Epoch 372/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5942 - auc: 0.7243 - ap: 0.2579\n",
            "Epoch 373/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5946 - auc: 0.7282 - ap: 0.2292\n",
            "Epoch 374/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5940 - auc: 0.7146 - ap: 0.2274\n",
            "Epoch 375/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5945 - auc: 0.7143 - ap: 0.2273\n",
            "Epoch 376/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5946 - auc: 0.7067 - ap: 0.2015\n",
            "Epoch 377/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5966 - auc: 0.7022 - ap: 0.2140\n",
            "Epoch 378/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5932 - auc: 0.7342 - ap: 0.2346\n",
            "Epoch 379/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5931 - auc: 0.6942 - ap: 0.2135\n",
            "Epoch 380/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5950 - auc: 0.7255 - ap: 0.2294\n",
            "Epoch 381/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5939 - auc: 0.7181 - ap: 0.2288\n",
            "Epoch 382/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5931 - auc: 0.7097 - ap: 0.2263\n",
            "Epoch 383/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5953 - auc: 0.7063 - ap: 0.2070\n",
            "Epoch 384/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5942 - auc: 0.7285 - ap: 0.2277\n",
            "Epoch 385/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5941 - auc: 0.7089 - ap: 0.2295\n",
            "Epoch 386/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5946 - auc: 0.7290 - ap: 0.2058\n",
            "Epoch 387/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5935 - auc: 0.7180 - ap: 0.1999\n",
            "Epoch 388/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5950 - auc: 0.7314 - ap: 0.1998\n",
            "Epoch 389/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5935 - auc: 0.7161 - ap: 0.2095\n",
            "Epoch 390/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5971 - auc: 0.7079 - ap: 0.2223\n",
            "Epoch 391/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5953 - auc: 0.7250 - ap: 0.2287\n",
            "Epoch 392/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5955 - auc: 0.7178 - ap: 0.2244\n",
            "Epoch 393/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5947 - auc: 0.7214 - ap: 0.2333\n",
            "Epoch 394/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5924 - auc: 0.7130 - ap: 0.2176\n",
            "Epoch 395/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5967 - auc: 0.7177 - ap: 0.2161\n",
            "Epoch 396/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5967 - auc: 0.7136 - ap: 0.2251\n",
            "Epoch 397/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5958 - auc: 0.7171 - ap: 0.2118\n",
            "Epoch 398/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5938 - auc: 0.7089 - ap: 0.1993\n",
            "Epoch 399/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5966 - auc: 0.7276 - ap: 0.2200\n",
            "Epoch 400/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5939 - auc: 0.7084 - ap: 0.2035\n",
            "Epoch 401/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5964 - auc: 0.7250 - ap: 0.2294\n",
            "Epoch 402/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5961 - auc: 0.7003 - ap: 0.1998\n",
            "Epoch 403/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5950 - auc: 0.7080 - ap: 0.2239\n",
            "Epoch 404/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5968 - auc: 0.7195 - ap: 0.2445\n",
            "Epoch 405/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5946 - auc: 0.7255 - ap: 0.2185\n",
            "Epoch 406/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5931 - auc: 0.7198 - ap: 0.2379\n",
            "Epoch 407/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5926 - auc: 0.7144 - ap: 0.2162\n",
            "Epoch 408/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5954 - auc: 0.7217 - ap: 0.2082\n",
            "Epoch 409/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5967 - auc: 0.7277 - ap: 0.2153\n",
            "Epoch 410/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5960 - auc: 0.7156 - ap: 0.2276\n",
            "Epoch 411/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5935 - auc: 0.7238 - ap: 0.2256\n",
            "Epoch 412/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5942 - auc: 0.7069 - ap: 0.2021\n",
            "Epoch 413/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5955 - auc: 0.7192 - ap: 0.2132\n",
            "Epoch 414/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5956 - auc: 0.7010 - ap: 0.2058\n",
            "Epoch 415/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5957 - auc: 0.7107 - ap: 0.2022\n",
            "Epoch 416/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5944 - auc: 0.7210 - ap: 0.2243\n",
            "Epoch 417/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5935 - auc: 0.7179 - ap: 0.2327\n",
            "Epoch 418/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5959 - auc: 0.7081 - ap: 0.2289\n",
            "Epoch 419/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5939 - auc: 0.7025 - ap: 0.2159\n",
            "Epoch 420/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5946 - auc: 0.7225 - ap: 0.2207\n",
            "Epoch 421/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5942 - auc: 0.7316 - ap: 0.2492\n",
            "Epoch 422/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5938 - auc: 0.7242 - ap: 0.2517\n",
            "Epoch 423/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5925 - auc: 0.7160 - ap: 0.2246\n",
            "Epoch 424/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5946 - auc: 0.7160 - ap: 0.2088\n",
            "Epoch 425/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5950 - auc: 0.7236 - ap: 0.2002\n",
            "Epoch 426/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5928 - auc: 0.7137 - ap: 0.2180\n",
            "Epoch 427/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5943 - auc: 0.7067 - ap: 0.2107\n",
            "Epoch 428/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5946 - auc: 0.7197 - ap: 0.2350\n",
            "Epoch 429/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5960 - auc: 0.6938 - ap: 0.2187\n",
            "Epoch 430/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5975 - auc: 0.7091 - ap: 0.2143\n",
            "Epoch 431/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5937 - auc: 0.7163 - ap: 0.2115\n",
            "Epoch 432/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5925 - auc: 0.7393 - ap: 0.2351\n",
            "Epoch 433/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5917 - auc: 0.7182 - ap: 0.2220\n",
            "Epoch 434/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5926 - auc: 0.7217 - ap: 0.2291\n",
            "Epoch 435/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5935 - auc: 0.7086 - ap: 0.2195\n",
            "Epoch 436/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5945 - auc: 0.7269 - ap: 0.2204\n",
            "Epoch 437/500\n",
            "9/9 [==============================] - 1s 160ms/step - loss: 0.5949 - auc: 0.6999 - ap: 0.1999\n",
            "Epoch 438/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5931 - auc: 0.7251 - ap: 0.2451\n",
            "Epoch 439/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5953 - auc: 0.7140 - ap: 0.2051\n",
            "Epoch 440/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5970 - auc: 0.7134 - ap: 0.2341\n",
            "Epoch 441/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5932 - auc: 0.7275 - ap: 0.2485\n",
            "Epoch 442/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5936 - auc: 0.7276 - ap: 0.2162\n",
            "Epoch 443/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5941 - auc: 0.7142 - ap: 0.2402\n",
            "Epoch 444/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5948 - auc: 0.7157 - ap: 0.2296\n",
            "Epoch 445/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5934 - auc: 0.7398 - ap: 0.2173\n",
            "Epoch 446/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5950 - auc: 0.7038 - ap: 0.2307\n",
            "Epoch 447/500\n",
            "9/9 [==============================] - 1s 153ms/step - loss: 0.5949 - auc: 0.7145 - ap: 0.2126\n",
            "Epoch 448/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5949 - auc: 0.7131 - ap: 0.2246\n",
            "Epoch 449/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5933 - auc: 0.7187 - ap: 0.2128\n",
            "Epoch 450/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5934 - auc: 0.7315 - ap: 0.2166\n",
            "Epoch 451/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5942 - auc: 0.7116 - ap: 0.2235\n",
            "Epoch 452/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5920 - auc: 0.7137 - ap: 0.2170\n",
            "Epoch 453/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5949 - auc: 0.7148 - ap: 0.2280\n",
            "Epoch 454/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5920 - auc: 0.6976 - ap: 0.1969\n",
            "Epoch 455/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5945 - auc: 0.7178 - ap: 0.2059\n",
            "Epoch 456/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5938 - auc: 0.7120 - ap: 0.2256\n",
            "Epoch 457/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5974 - auc: 0.7175 - ap: 0.2257\n",
            "Epoch 458/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5921 - auc: 0.7221 - ap: 0.2176\n",
            "Epoch 459/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5926 - auc: 0.7070 - ap: 0.2106\n",
            "Epoch 460/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5931 - auc: 0.7216 - ap: 0.2171\n",
            "Epoch 461/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5938 - auc: 0.7250 - ap: 0.2143\n",
            "Epoch 462/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5932 - auc: 0.7210 - ap: 0.2382\n",
            "Epoch 463/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5954 - auc: 0.7095 - ap: 0.2032\n",
            "Epoch 464/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5940 - auc: 0.7295 - ap: 0.2280\n",
            "Epoch 465/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5947 - auc: 0.7059 - ap: 0.2158\n",
            "Epoch 466/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5905 - auc: 0.7232 - ap: 0.2161\n",
            "Epoch 467/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5979 - auc: 0.7196 - ap: 0.2134\n",
            "Epoch 468/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5942 - auc: 0.7202 - ap: 0.2175\n",
            "Epoch 469/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5977 - auc: 0.7270 - ap: 0.1997\n",
            "Epoch 470/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5952 - auc: 0.7208 - ap: 0.2526\n",
            "Epoch 471/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5940 - auc: 0.7215 - ap: 0.2211\n",
            "Epoch 472/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5931 - auc: 0.7216 - ap: 0.2431\n",
            "Epoch 473/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5963 - auc: 0.7211 - ap: 0.2200\n",
            "Epoch 474/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5943 - auc: 0.7256 - ap: 0.2095\n",
            "Epoch 475/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5944 - auc: 0.7132 - ap: 0.2257\n",
            "Epoch 476/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5893 - auc: 0.7215 - ap: 0.2144\n",
            "Epoch 477/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5935 - auc: 0.7178 - ap: 0.2300\n",
            "Epoch 478/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5959 - auc: 0.7208 - ap: 0.2220\n",
            "Epoch 479/500\n",
            "9/9 [==============================] - 1s 157ms/step - loss: 0.5936 - auc: 0.7221 - ap: 0.2176\n",
            "Epoch 480/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5919 - auc: 0.7128 - ap: 0.2157\n",
            "Epoch 481/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5927 - auc: 0.7183 - ap: 0.2493\n",
            "Epoch 482/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5938 - auc: 0.7060 - ap: 0.2223\n",
            "Epoch 483/500\n",
            "9/9 [==============================] - 1s 159ms/step - loss: 0.5925 - auc: 0.6980 - ap: 0.2056\n",
            "Epoch 484/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5917 - auc: 0.7150 - ap: 0.2381\n",
            "Epoch 485/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5957 - auc: 0.7119 - ap: 0.2372\n",
            "Epoch 486/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5900 - auc: 0.7220 - ap: 0.2159\n",
            "Epoch 487/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5935 - auc: 0.7272 - ap: 0.2428\n",
            "Epoch 488/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5940 - auc: 0.7363 - ap: 0.2267\n",
            "Epoch 489/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5936 - auc: 0.7069 - ap: 0.2076\n",
            "Epoch 490/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5963 - auc: 0.7062 - ap: 0.2266\n",
            "Epoch 491/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5947 - auc: 0.7273 - ap: 0.2347\n",
            "Epoch 492/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5937 - auc: 0.7216 - ap: 0.2146\n",
            "Epoch 493/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5933 - auc: 0.6968 - ap: 0.2011\n",
            "Epoch 494/500\n",
            "9/9 [==============================] - 1s 156ms/step - loss: 0.5950 - auc: 0.7244 - ap: 0.2343\n",
            "Epoch 495/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5939 - auc: 0.6986 - ap: 0.1974\n",
            "Epoch 496/500\n",
            "9/9 [==============================] - 1s 154ms/step - loss: 0.5938 - auc: 0.7226 - ap: 0.2281\n",
            "Epoch 497/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5937 - auc: 0.7048 - ap: 0.2113\n",
            "Epoch 498/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5944 - auc: 0.7294 - ap: 0.2459\n",
            "Epoch 499/500\n",
            "9/9 [==============================] - 1s 155ms/step - loss: 0.5940 - auc: 0.7091 - ap: 0.2214\n",
            "Epoch 500/500\n",
            "9/9 [==============================] - 1s 158ms/step - loss: 0.5932 - auc: 0.7352 - ap: 0.2220\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd2bb25df98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3REsfg0X5LA9",
        "outputId": "6c1e03ac-072e-40d5-fe38-5d69f42b9ccb"
      },
      "source": [
        "x_predict = model_rsrae.predict(x)[2]\n",
        "\n",
        "auc = roc_auc_score(y, cosine_similarity(x_predict, x))\n",
        "ap = average_precision_score(y, cosine_similarity(x_predict, x))\n",
        "                                    \n",
        "print(\"auc = \", auc)\n",
        "print(\"ap = \", ap)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "auc =  0.8196600147819659\n",
            "ap =  0.29471878860550343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOkxnvMS6hFK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}