{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA+NMF_fastText_experiments_ipynb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1pm-I9sTCln"
      },
      "source": [
        "## Предоработка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RNP065_S3qD",
        "outputId": "1d106182-9006-4044-c7b2-10b4458d0c38"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.parsing.preprocessing import strip_short\n",
        "from gensim.parsing.preprocessing import strip_non_alphanum\n",
        "from gensim.parsing.preprocessing import strip_numeric\n",
        "from gensim.utils import tokenize\n",
        "import nltk; nltk.download('wordnet')\n",
        "import nltk; nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    \"\"\"Удаление html tags из текста.\"\"\"\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text(separator=\" \")\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def preprocess_text(text, token=False):\n",
        "    text = text.lower()\n",
        "    text = strip_html_tags(text)  # удаление html tags\n",
        "    text = strip_non_alphanum(text) # заменили все небуквенные символы на пробел\n",
        "    text = strip_numeric(text) # удалили все цифры\n",
        "    text = remove_stopwords(text) # удалили все стоп-слова с gensim\n",
        "    text = strip_short(text, minsize=3) # удалили слова из <3 символов\n",
        "    word_list = list(tokenize(text, deacc=True)) # токенизация, deacc - избавляет от ударений\n",
        "    word_list = [w for w in word_list if not w in stop_words] # удалили стоп слова с nltk\n",
        "    word_list = [WordNetLemmatizer().lemmatize(word) for word in word_list] # лемматизация\n",
        "    if (token):\n",
        "        return word_list\n",
        "    else:\n",
        "        return ' '.join(w for w in word_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls0gJhW5TL9e"
      },
      "source": [
        "## Ранжирование"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUAObDehTSRB"
      },
      "source": [
        "from gensim.matutils import jensen_shannon\n",
        "\n",
        "\n",
        "def GetListPopularTopics(lda_model, bow_corpus):\n",
        "    \"\"\"Cоставляет ранжированный список тем, остортированных по убыванию\n",
        "    популярности. Возвращает ранжированный список индексов тем.\"\"\"\n",
        "\n",
        "    topTopicsPopular = [0 for i in range(lda_model.num_topics)]\n",
        "\n",
        "    for bow in bow_corpus:  # идем по всем документам\n",
        "        # Получаем распределение тем для каждого документа\n",
        "        distTopicsInDocument = lda_model.get_document_topics(bow,\n",
        "                                                             minimum_probability=0)\n",
        "\n",
        "        for (idx, pr) in distTopicsInDocument:\n",
        "            topTopicsPopular[idx] += pr\n",
        "\n",
        "    topTopicsPopular = [(i, topTopicsPopular[i]) for i in\n",
        "                        range(len(topTopicsPopular))]\n",
        "    # сортируем темы по популярности\n",
        "    topTopicsPopular = sorted(topTopicsPopular, key=lambda pair: pair[1],\n",
        "                              reverse=True)\n",
        "\n",
        "    return [x[0] for x in topTopicsPopular]\n",
        "\n",
        "\n",
        "def GetListPrimTopics(lda_model, top_popular_topics):\n",
        "    topics = [lda_model.get_topic_terms(i, len(lda_model.id2word)) for i in range(lda_model.num_topics)]\n",
        "    for i in range(lda_model.num_topics):\n",
        "        topics[i] = sorted(topics[i])\n",
        "\n",
        "    edge_dict = dict()\n",
        "    for v1 in range(lda_model.num_topics):\n",
        "        for v2 in range(lda_model.num_topics):\n",
        "            edge_dict[(v1, v2)] = jensen_shannon(topics[v1], topics[v2])\n",
        "\n",
        "    start_vertex = top_popular_topics[0] # назначаем самую популярную тему стартовой\n",
        "\n",
        "    rankingTopics = []\n",
        "\n",
        "    selected = set()\n",
        "    selected.add(start_vertex)\n",
        "    rankingTopics.append(start_vertex)\n",
        "\n",
        "    unselected = set()\n",
        "    for i in range(lda_model.num_topics):\n",
        "        if(i != start_vertex):\n",
        "            unselected.add(i)\n",
        "\n",
        "    ans = 0 # хранит вес минимального остовного дерева\n",
        "\n",
        "    # алгоритм Прима\n",
        "    while(len(selected) != lda_model.num_topics):\n",
        "        newV1 = -1\n",
        "        newV2 = -1\n",
        "        mx = -1\n",
        "        for v1 in selected:\n",
        "            for v2 in unselected:\n",
        "                if (mx == -1 or edge_dict[(v1, v2)] < mx):\n",
        "                    mx = edge_dict[(v1, v2)]\n",
        "                    newV1 = v1\n",
        "                    newV2 = v2\n",
        "        selected.add(newV2)\n",
        "        unselected.discard(newV2)\n",
        "        ans += mx\n",
        "        rankingTopics.append(newV2)\n",
        "    return rankingTopics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuzYps2eTzsb"
      },
      "source": [
        "## Контекст с fastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zu7EHOaTS2A",
        "outputId": "57062683-699a-4b38-f7fb-dbf7275e0db5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import wordnet_ic\n",
        "\n",
        "nltk.download('wordnet_ic')\n",
        "\n",
        "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
        "\n",
        "\n",
        "def GetListWordsTopic(lda_model, dictionary, topicid, topn=10):\n",
        "    \"\"\"Функция, которая по модели, словарю и номеру темы возвращает список\n",
        "    topn характеризующих эту тему слов\"\"\"\n",
        "    words = lda_model.get_topic_terms(topicid=topicid, topn=topn)\n",
        "    id = []\n",
        "    for i in words:\n",
        "        id.append(i[0])\n",
        "    words = [dictionary[i] for i in id]\n",
        "    return words\n",
        "\n",
        "\n",
        "def simWN(word1, word2):\n",
        "    sim1 = sim(word1, word2, wn.wup_similarity)\n",
        "    sim2 = 0\n",
        "    divider = 2\n",
        "    try:\n",
        "        sim2 = sim(word1, word2, wn.lin_similarity, ic=semcor_ic)\n",
        "    except:\n",
        "        divider = 1\n",
        "    return (sim1 + sim2) / divider\n",
        "\n",
        "\n",
        "def sim(word1, word2, fun, flag_print=False, ic=None):\n",
        "    syns1 = wn.synsets(word1)\n",
        "    syns2 = wn.synsets(word2)\n",
        "    if (syns1 == None or syns2 == None):\n",
        "        return 0\n",
        "    mx = 0\n",
        "    for i in syns1:\n",
        "        for j in syns2:\n",
        "            tmp = None\n",
        "            if (ic != None):  # если вычисляем сходство на основе вектора толкования\n",
        "                if (i.pos() == j.pos()):  # test for equal part of speech (pos)\n",
        "                    tmp = fun(i, j, ic)\n",
        "            else:  # вычисление сходства посредством подсчета расстояний в графе\n",
        "                tmp = fun(i, j)\n",
        "            if (tmp != None):\n",
        "                mx = max(tmp, mx)\n",
        "    if (flag_print):\n",
        "        print(\"Similarity between\", word1, \"and\", word2, \":\", mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def GetTopInContext(rankingTopics, lda_model, dictionary,\n",
        "                    num_words):\n",
        "    m = 2 / 3\n",
        "    N_normalTopics = int(lda_model.num_topics * m)\n",
        "    normalTopics = rankingTopics[0:N_normalTopics + 1]\n",
        "    anomalTopics = rankingTopics[N_normalTopics + 1:]\n",
        "\n",
        "    # N_wordsСharacteristic число слов, которое будем использовать, чтобы характеризовать тему\n",
        "    wordOfTopics = []\n",
        "    for i in range(lda_model.num_topics):\n",
        "        wordOfTopics.append(\n",
        "            GetListWordsTopic(lda_model, dictionary, i, num_words))\n",
        "\n",
        "    # Задаем меру схожести\n",
        "    SimFunction = simWN\n",
        "\n",
        "    measureOfAnomaly = []\n",
        "    for an_i in anomalTopics:  # для каждой потенциально аномальной темы\n",
        "        r_total = 0\n",
        "        for n_i in normalTopics:  # берем в пару очередную нормальную тему\n",
        "            r = 0\n",
        "            for an_word in wordOfTopics[\n",
        "                an_i]:  # перебираем все слова аномальной темы\n",
        "                for n_word in wordOfTopics[\n",
        "                    n_i]:  # перебираем все слова номральной темы\n",
        "                    r += SimFunction(an_word, n_word)\n",
        "            r_total += r\n",
        "        measureOfAnomaly.append((an_i, r_total))\n",
        "    measureOfAnomaly = sorted(measureOfAnomaly, key=lambda pair: pair[1],\n",
        "                              reverse=(SimFunction == simWN))\n",
        "    return [x[0] for x in measureOfAnomaly]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGtOf-ZPUamt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae5c1f9d-8bbf-4f9d-f03b-6cde747dab92"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKqsmx80Xzb-"
      },
      "source": [
        "## Подсчет anomal_score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mICzxYt9XppC"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def GetPredict(model, bow_corpus, anomal_topics):\n",
        "    \"\"\"Для каждого документа оценивает его аномальность. Возвращает список\n",
        "    из anomal_score.\"\"\"\n",
        "    y_predict = []\n",
        "    for (i, sample) in enumerate(bow_corpus):\n",
        "        anomal_score = 0\n",
        "        distTopicsInDocument = model.get_document_topics(sample, minimum_probability=0)\n",
        "        for idx, pr in distTopicsInDocument:\n",
        "            if(idx in anomal_topics):\n",
        "                anomal_score += pr\n",
        "        y_predict.append(anomal_score)\n",
        "    X = np.array(y_predict)\n",
        "    X_std = (X - X.min()) / (X.max() - X.min()) # [0:1]\n",
        "    return X_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-mEirx5X3DL"
      },
      "source": [
        "## Эксперименты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUnLdzHYot1Q"
      },
      "source": [
        "# LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk036zZfomf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff06e4e-612d-4fa8-cbb3-1a91c752cff6"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaMulticore\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from gensim.models import TfidfModel\n",
        "\n",
        "c = 0.1  # отношение количества аномальных экземпляров к нормальным\n",
        "\n",
        "anomal_categories = [\n",
        "    'comp.os.ms-windows.misc', \n",
        "    'talk.politics.mideast',\n",
        "    'soc.religion.christian',\n",
        "    'talk.politics.mideast',\n",
        "    'misc.forsale',\n",
        "    'sci.med',\n",
        "    'sci.space',\n",
        "    'comp.sys.ibm.pc.hardware',\n",
        "    'talk.politics.mideast',\n",
        "    'comp.windows.x',\n",
        "    'comp.sys.ibm.pc.hardware',\n",
        "    'comp.windows.x',\n",
        "    'comp.sys.ibm.pc.hardware',\n",
        "    'sci.electronics',\n",
        "    'soc.religion.christian',\n",
        "    'soc.religion.christian',\n",
        "    'comp.graphics',\n",
        "    'comp.windows.x',\n",
        "    'talk.politics.misc',\n",
        "    'sci.crypt',\n",
        "    'talk.politics.mideast',\n",
        "    'talk.politics.mideast',\n",
        "    'talk.politics.mideast',\n",
        "    'soc.religion.christian',\n",
        "    'sci.space',\n",
        "    'talk.politics.mideast',\n",
        "    'sci.crypt',\n",
        "    'talk.politics.mideast',\n",
        "    'comp.windows.x',\n",
        "    'rec.sport.baseball',\n",
        "    'comp.windows.x',\n",
        "    'rec.sport.hockey',\n",
        "    'comp.sys.ibm.pc.hardware',\n",
        "    'comp.windows.x',\n",
        "    'sci.crypt',\n",
        "]\n",
        "\n",
        "normal_categories = [\n",
        "                     'rec.autos',\n",
        "                     'sci.electronics',\n",
        "                     'rec.autos',\n",
        "                     'sci.space',\n",
        "                     'rec.motorcycles',\n",
        "                     'talk.politics.mideast',\n",
        "                     'talk.politics.mideast',\n",
        "                     'talk.politics.misc',\n",
        "                     'soc.religion.christian',\n",
        "                     'talk.politics.misc',\n",
        "                     'alt.atheism',\n",
        "                     'alt.atheism',\n",
        "                     'talk.politics.misc',\n",
        "                     'soc.religion.christian',\n",
        "                     'rec.sport.baseball',\n",
        "                     'comp.graphics',\n",
        "                     'soc.religion.christian',\n",
        "                     'soc.religion.christian',\n",
        "                     'sci.electronics',\n",
        "                     'rec.autos',\n",
        "                     'sci.crypt',\n",
        "                     'sci.med',\n",
        "                     'rec.motorcycles',\n",
        "                     'sci.space',\n",
        "                     'soc.religion.christian',\n",
        "                     'comp.graphics',\n",
        "                     'rec.sport.hockey',\n",
        "                     'comp.sys.ibm.pc.hardware',\n",
        "                     'rec.sport.baseball',\n",
        "                     'talk.politics.mideast',\n",
        "                     'rec.autos',\n",
        "                     'sci.space',\n",
        "                     'soc.religion.christian',\n",
        "                     'rec.sport.hockey',\n",
        "                     'rec.sport.baseball'\n",
        "]\n",
        "    \n",
        "\n",
        "\n",
        "categories = ['alt.atheism',\n",
        "                'comp.graphics',\n",
        "                'comp.os.ms-windows.misc',\n",
        "                'comp.sys.ibm.pc.hardware',\n",
        "                'comp.sys.mac.hardware',\n",
        "                'comp.windows.x',\n",
        "                'misc.forsale',\n",
        "                'rec.autos',\n",
        "                'rec.motorcycles',\n",
        "                'rec.sport.baseball',\n",
        "                'rec.sport.hockey',\n",
        "                'sci.crypt',\n",
        "                'sci.electronics',\n",
        "                'sci.med',\n",
        "                'sci.space',\n",
        "                'soc.religion.christian',\n",
        "                'talk.politics.guns',\n",
        "                'talk.politics.mideast',\n",
        "                'talk.politics.misc',\n",
        "                'talk.religion.misc']\n",
        "\n",
        "experimant_cnt = 0\n",
        "all_experiments = len(normal_categories) # len(categories) * (len(categories) - 1)\n",
        "auc_list_content = []\n",
        "auc_list_context = []\n",
        "\n",
        "# Формирование словаря с категориями\n",
        "dataset = {}\n",
        "for cat in categories:\n",
        "    dataset[cat] = fetch_20newsgroups(subset='all', categories=[cat],\n",
        "                            shuffle=True, random_state=123,\n",
        "                            remove=('headers', 'footers'), return_X_y=True)[0]\n",
        "\n",
        "# Перебираем пары категорий\n",
        "# for c1 in categories:\n",
        "#     for c2 in categories:\n",
        "for c1, c2 in zip(normal_categories, anomal_categories):\n",
        "    if c1 != c2:\n",
        "        experimant_cnt += 1\n",
        "        # Формирование нормальной и аномальной выборок\n",
        "        normal_data = dataset[c1]\n",
        "        anomal_data = dataset[c2][:min(int(c * len(normal_data)) + 1, len(dataset[c2]))]\n",
        "        y = np.array(\n",
        "            [False] * len(normal_data) + [True] * len(anomal_data))\n",
        "        normal_data = [preprocess_text(text, token=True) for text in normal_data]\n",
        "        anomal_data = [preprocess_text(text, token=True) for text in anomal_data]\n",
        "        all_data = normal_data + anomal_data\n",
        "\n",
        "        # Формирование словаря\n",
        "        dictionary = corpora.Dictionary(all_data)\n",
        "        # Оставляем слова, встречающиеся >= no_below документах\n",
        "        # Оставляем слова, встречающиеся <= чем в no_above документах\n",
        "        # dictionary.filter_extremes(no_below=5, no_above=0.4,\n",
        "                                    # keep_n=None)\n",
        "        # Назачение новых индексов словам, минуя пропуски\n",
        "        # dictionary.compactify()\n",
        "\n",
        "        # Создание мешка слов\n",
        "        bag_of_words = [dictionary.doc2bow(doc) for doc in all_data] # convert corpus to Bag of Words\n",
        "\n",
        "        # model = TfidfModel(bag_of_words)  # fit model\n",
        "        # vector = model[bag_of_words]   # apply model to the first corpus document\n",
        "\n",
        "        # Тематическое моделирование\n",
        "        lda_model = LdaMulticore(corpus=bag_of_words,\n",
        "                                    num_topics=10,\n",
        "                                    random_state=123,\n",
        "                                    id2word=dictionary,\n",
        "                                    passes=10,\n",
        "                                    workers=2)\n",
        "        # Ранжирование тем\n",
        "        topics_popular = GetListPopularTopics(lda_model, bag_of_words)\n",
        "        # topics_prim = GetListPrimTopics(lda_model, topics_popular)\n",
        "\n",
        "        # Формирование аномальных тем\n",
        "        anomal_topics_content = topics_popular[-3:]\n",
        "        anomal_topics_context = GetTopInContext(topics_popular,\n",
        "                                                lda_model,\n",
        "                                                dictionary,\n",
        "                                                50)[-3:]\n",
        "\n",
        "        y_predict_content = GetPredict(lda_model, bag_of_words,\n",
        "                                anomal_topics_content)\n",
        "        y_predict_context = GetPredict(lda_model, bag_of_words,\n",
        "                                anomal_topics_context)\n",
        "\n",
        "        # y_predict_content /= y_predict_content.max()\n",
        "        # y_predict_context /= y_predict_context.max()\n",
        "\n",
        "        auc_content = roc_auc_score(y, y_predict_content)\n",
        "        auc_context = roc_auc_score(y, y_predict_context)\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        print(\"Эксперимент №{}/{}  с normal = {}, anomal = {}\".format(\n",
        "            experimant_cnt, all_experiments, c1, c2))\n",
        "        print(\"Контент: roc_auc_score = {}\".format(auc_content))\n",
        "        print(\"Контекст: roc_auc_score = {}\".format(auc_context))\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        auc_list_content.append(auc_content)\n",
        "        auc_list_context.append(auc_context)\n",
        "\n",
        "auc_np_content = np.array(auc_list_content)\n",
        "auc_np_context = np.array(auc_list_context)\n",
        "print(\"*\" * 50)\n",
        "print(\"Медиана auc_content = {}\".format(np.median(auc_np_content)))\n",
        "print(\"Среднее auc_content = {}\".format(np.mean(auc_np_content)))\n",
        "print(\"LDA Медиана auc_context = {}\".format(np.median(auc_np_context)))\n",
        "print(\"LDA Среднее auc_context = {}\".format(np.mean(auc_np_context)))\n",
        "print(\"*\" * 50)\n",
        "print(\"LDA Эксперименты завершены!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Эксперимент №1/35  с normal = rec.autos, anomal = comp.os.ms-windows.misc\n",
            "Контент: roc_auc_score = 0.7590505050505051\n",
            "Контекст: roc_auc_score = 0.75910101010101\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №2/35  с normal = sci.electronics, anomal = talk.politics.mideast\n",
            "Контент: roc_auc_score = 0.760203662642687\n",
            "Контекст: roc_auc_score = 0.7601626016260162\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №3/35  с normal = rec.autos, anomal = soc.religion.christian\n",
            "Контент: roc_auc_score = 0.7602222222222221\n",
            "Контекст: roc_auc_score = 0.7604040404040404\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №4/35  с normal = sci.space, anomal = talk.politics.mideast\n",
            "Контент: roc_auc_score = 0.7608301863621012\n",
            "Контекст: roc_auc_score = 0.7609120587843993\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №5/35  с normal = rec.motorcycles, anomal = misc.forsale\n",
            "Контент: roc_auc_score = 0.7624899598393574\n",
            "Контекст: roc_auc_score = 0.7622088353413654\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №6/35  с normal = talk.politics.mideast, anomal = sci.med\n",
            "Контент: roc_auc_score = 0.7652967525195968\n",
            "Контекст: roc_auc_score = 0.7653415453527437\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №7/35  с normal = talk.politics.mideast, anomal = sci.space\n",
            "Контент: roc_auc_score = 0.7673796192609182\n",
            "Контекст: roc_auc_score = 0.767424412094065\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №8/35  с normal = talk.politics.misc, anomal = comp.sys.ibm.pc.hardware\n",
            "Контент: roc_auc_score = 0.7782547559966915\n",
            "Контекст: roc_auc_score = 0.7785194375516955\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №9/35  с normal = soc.religion.christian, anomal = talk.politics.mideast\n",
            "Контент: roc_auc_score = 0.7685857572718153\n",
            "Контекст: roc_auc_score = 0.7684353059177532\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №10/35  с normal = talk.politics.misc, anomal = comp.windows.x\n",
            "Контент: roc_auc_score = 0.7686517783291976\n",
            "Контекст: roc_auc_score = 0.7684863523573202\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №11/35  с normal = alt.atheism, anomal = comp.sys.ibm.pc.hardware\n",
            "Контент: roc_auc_score = 0.7721605131414268\n",
            "Контекст: roc_auc_score = 0.7721448685857322\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №12/35  с normal = alt.atheism, anomal = comp.windows.x\n",
            "Контент: roc_auc_score = 0.7748122653316646\n",
            "Контекст: roc_auc_score = 0.7745619524405507\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №13/35  с normal = talk.politics.misc, anomal = comp.sys.ibm.pc.hardware\n",
            "Контент: roc_auc_score = 0.7782547559966915\n",
            "Контекст: roc_auc_score = 0.7785194375516955\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №14/35  с normal = soc.religion.christian, anomal = sci.electronics\n",
            "Контент: roc_auc_score = 0.7873821464393179\n",
            "Контекст: roc_auc_score = 0.7874022066198595\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №15/35  с normal = rec.sport.baseball, anomal = soc.religion.christian\n",
            "Контент: roc_auc_score = 0.787625754527163\n",
            "Контекст: roc_auc_score = 0.788138832997988\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №16/35  с normal = comp.graphics, anomal = soc.religion.christian\n",
            "Контент: roc_auc_score = 0.8166935839083835\n",
            "Контекст: roc_auc_score = 0.8167984562787088\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №17/35  с normal = soc.religion.christian, anomal = comp.graphics\n",
            "Контент: roc_auc_score = 0.8174423269809428\n",
            "Контекст: roc_auc_score = 0.8172517552657974\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №18/35  с normal = soc.religion.christian, anomal = comp.windows.x\n",
            "Контент: roc_auc_score = 0.8184653961885656\n",
            "Контекст: roc_auc_score = 0.8183650952858575\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №19/35  с normal = sci.electronics, anomal = talk.politics.misc\n",
            "Контент: roc_auc_score = 0.8195881580027922\n",
            "Контекст: roc_auc_score = 0.8196086885111277\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №20/35  с normal = rec.autos, anomal = sci.crypt\n",
            "Контент: roc_auc_score = 0.8287676767676767\n",
            "Контекст: roc_auc_score = 0.8267777777777777\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №21/35  с normal = sci.crypt, anomal = talk.politics.mideast\n",
            "Контент: roc_auc_score = 0.831816347124117\n",
            "Контекст: roc_auc_score = 0.832300706357215\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №22/35  с normal = sci.med, anomal = talk.politics.mideast\n",
            "Контент: roc_auc_score = 0.8351414141414141\n",
            "Контекст: roc_auc_score = 0.836141414141414\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №23/35  с normal = rec.motorcycles, anomal = talk.politics.mideast\n",
            "Контент: roc_auc_score = 0.8393674698795182\n",
            "Контекст: roc_auc_score = 0.8393473895582331\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №24/35  с normal = sci.space, anomal = soc.religion.christian\n",
            "Контент: roc_auc_score = 0.8410549261613092\n",
            "Контекст: roc_auc_score = 0.841044692108522\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №25/35  с normal = soc.religion.christian, anomal = sci.space\n",
            "Контент: roc_auc_score = 0.845727181544634\n",
            "Контекст: roc_auc_score = 0.8457271815446339\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №26/35  с normal = comp.graphics, anomal = talk.politics.mideast\n",
            "Контент: roc_auc_score = 0.8509868490047613\n",
            "Контекст: roc_auc_score = 0.8509868490047613\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №27/35  с normal = rec.sport.hockey, anomal = sci.crypt\n",
            "Контент: roc_auc_score = 0.858938938938939\n",
            "Контекст: roc_auc_score = 0.8589289289289289\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №28/35  с normal = comp.sys.ibm.pc.hardware, anomal = talk.politics.mideast\n",
            "Контент: roc_auc_score = 0.5188956777551482\n",
            "Контекст: roc_auc_score = 0.5188853915941491\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №29/35  с normal = rec.sport.baseball, anomal = comp.windows.x\n",
            "Контент: roc_auc_score = 0.8644265593561368\n",
            "Контекст: roc_auc_score = 0.8643561368209256\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №30/35  с normal = talk.politics.mideast, anomal = rec.sport.baseball\n",
            "Контент: roc_auc_score = 0.8683090705487123\n",
            "Контекст: roc_auc_score = 0.8677827547592385\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №31/35  с normal = rec.autos, anomal = comp.windows.x\n",
            "Контент: roc_auc_score = 0.8870808080808079\n",
            "Контекст: roc_auc_score = 0.887010101010101\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №32/35  с normal = sci.space, anomal = rec.sport.hockey\n",
            "Контент: roc_auc_score = 0.8999826021102617\n",
            "Контекст: roc_auc_score = 0.9000440064269851\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №33/35  с normal = soc.religion.christian, anomal = comp.sys.ibm.pc.hardware\n",
            "Контент: roc_auc_score = 0.9045837512537613\n",
            "Контекст: roc_auc_score = 0.9045636910732195\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №34/35  с normal = rec.sport.hockey, anomal = comp.windows.x\n",
            "Контент: roc_auc_score = 0.9177577577577577\n",
            "Контекст: roc_auc_score = 0.9176976976976977\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "Эксперимент №35/35  с normal = rec.sport.baseball, anomal = sci.crypt\n",
            "Контент: roc_auc_score = 0.9228068410462777\n",
            "Контекст: roc_auc_score = 0.9227364185110665\n",
            "--------------------------------------------------\n",
            "**************************************************\n",
            "Медиана auc_content = 0.8174423269809428\n",
            "Среднее auc_content = 0.8096866848995221\n",
            "LDA Медиана auc_context = 0.8172517552657974\n",
            "LDA Среднее auc_context = 0.8096605151537886\n",
            "**************************************************\n",
            "LDA Эксперименты завершены!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC5I60IIQRfa"
      },
      "source": [
        "## NMF-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5G4KFPNQRfd"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models.nmf import Nmf\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from gensim.models import TfidfModel\n",
        "\n",
        "c = 0.1  # отношение количества аномальных экземпляров к нормальным\n",
        "\n",
        "anomal_categories = [\n",
        "    'comp.os.ms-windows.misc', \n",
        "    'talk.politics.mideast',\n",
        "    'soc.religion.christian',\n",
        "    'talk.politics.mideast',\n",
        "    'misc.forsale',\n",
        "    'sci.med',\n",
        "    'sci.space',\n",
        "    'comp.sys.ibm.pc.hardware',\n",
        "    'talk.politics.mideast',\n",
        "    'comp.windows.x',\n",
        "    'comp.sys.ibm.pc.hardware',\n",
        "    'comp.windows.x',\n",
        "    'comp.sys.ibm.pc.hardware',\n",
        "    'sci.electronics',\n",
        "    'soc.religion.christian',\n",
        "    'soc.religion.christian',\n",
        "    'comp.graphics',\n",
        "    'comp.windows.x',\n",
        "    'talk.politics.misc',\n",
        "    'sci.crypt',\n",
        "    'talk.politics.mideast',\n",
        "    'talk.politics.mideast',\n",
        "    'talk.politics.mideast',\n",
        "    'soc.religion.christian',\n",
        "    'sci.space',\n",
        "    'talk.politics.mideast',\n",
        "    'sci.crypt',\n",
        "    'talk.politics.mideast',\n",
        "    'comp.windows.x',\n",
        "    'rec.sport.baseball',\n",
        "    'comp.windows.x',\n",
        "    'rec.sport.hockey',\n",
        "    'comp.sys.ibm.pc.hardware',\n",
        "    'comp.windows.x',\n",
        "    'sci.crypt',\n",
        "]\n",
        "\n",
        "normal_categories = [\n",
        "                     'rec.autos',\n",
        "                     'sci.electronics',\n",
        "                     'rec.autos',\n",
        "                     'sci.space',\n",
        "                     'rec.motorcycles',\n",
        "                     'talk.politics.mideast',\n",
        "                     'talk.politics.mideast',\n",
        "                     'talk.politics.misc',\n",
        "                     'soc.religion.christian',\n",
        "                     'talk.politics.misc',\n",
        "                     'alt.atheism',\n",
        "                     'alt.atheism',\n",
        "                     'talk.politics.misc',\n",
        "                     'soc.religion.christian',\n",
        "                     'rec.sport.baseball',\n",
        "                     'comp.graphics',\n",
        "                     'soc.religion.christian',\n",
        "                     'soc.religion.christian',\n",
        "                     'sci.electronics',\n",
        "                     'rec.autos',\n",
        "                     'sci.crypt',\n",
        "                     'sci.med',\n",
        "                     'rec.motorcycles',\n",
        "                     'sci.space',\n",
        "                     'soc.religion.christian',\n",
        "                     'comp.graphics',\n",
        "                     'rec.sport.hockey',\n",
        "                     'comp.sys.ibm.pc.hardware',\n",
        "                     'rec.sport.baseball',\n",
        "                     'talk.politics.mideast',\n",
        "                     'rec.autos',\n",
        "                     'sci.space',\n",
        "                     'soc.religion.christian',\n",
        "                     'rec.sport.hockey',\n",
        "                     'rec.sport.baseball'\n",
        "]\n",
        "    \n",
        "\n",
        "\n",
        "categories = ['alt.atheism',\n",
        "                'comp.graphics',\n",
        "                'comp.os.ms-windows.misc',\n",
        "                'comp.sys.ibm.pc.hardware',\n",
        "                'comp.sys.mac.hardware',\n",
        "                'comp.windows.x',\n",
        "                'misc.forsale',\n",
        "                'rec.autos',\n",
        "                'rec.motorcycles',\n",
        "                'rec.sport.baseball',\n",
        "                'rec.sport.hockey',\n",
        "                'sci.crypt',\n",
        "                'sci.electronics',\n",
        "                'sci.med',\n",
        "                'sci.space',\n",
        "                'soc.religion.christian',\n",
        "                'talk.politics.guns',\n",
        "                'talk.politics.mideast',\n",
        "                'talk.politics.misc',\n",
        "                'talk.religion.misc']\n",
        "\n",
        "experimant_cnt = 0\n",
        "all_experiments = len(anomal_categories) # len(categories) * (len(anomal_categories) - 1)\n",
        "auc_list_context = []\n",
        "\n",
        "# Формирование словаря с категориями\n",
        "dataset = {}\n",
        "for cat in categories:\n",
        "    dataset[cat] = fetch_20newsgroups(subset='all', categories=[cat],\n",
        "                            shuffle=True, random_state=123,\n",
        "                            remove=('headers', 'footers'), return_X_y=True)[0]\n",
        "\n",
        "# Перебираем пары категорий\n",
        "# for c1 in categories:\n",
        "#     for c2 in categories:\n",
        "for c1, c2 in zip(normal_categories, anomal_categories):\n",
        "    if c1 != c2:\n",
        "        experimant_cnt += 1\n",
        "        # Формирование нормальной и аномальной выборок\n",
        "        normal_data = dataset[c1]\n",
        "        anomal_data = dataset[c2][:min(int(c * len(normal_data)) + 1, len(dataset[c2]))]\n",
        "        y = np.array(\n",
        "            [False] * len(normal_data) + [True] * len(anomal_data))\n",
        "        normal_data = [preprocess_text(text, token=True) for text in normal_data]\n",
        "        anomal_data = [preprocess_text(text, token=True) for text in anomal_data]\n",
        "        all_data = normal_data + anomal_data\n",
        "\n",
        "        # Формирование словаря\n",
        "        dictionary = corpora.Dictionary(all_data)\n",
        "        # Оставляем слова, встречающиеся >= no_below документах\n",
        "        # Оставляем слова, встречающиеся <= чем в no_above документах\n",
        "        # dictionary.filter_extremes(no_below=5, no_above=0.4,\n",
        "                                    # keep_n=None)\n",
        "        # Назачение новых индексов словам, минуя пропуски\n",
        "        # dictionary.compactify()\n",
        "\n",
        "        # Создание мешка слов\n",
        "        bag_of_words = [dictionary.doc2bow(doc) for doc in all_data] # convert corpus to Bag of Words\n",
        "\n",
        "        # model = TfidfModel(bag_of_words)  # fit model\n",
        "        # vector = model[bag_of_words]   # apply model to the first corpus document\n",
        "\n",
        "        # Тематическое моделирование\n",
        "        lda_model = Nmf(corpus=bag_of_words,\n",
        "                            num_topics=10,\n",
        "                            random_state=123,\n",
        "                            id2word=dictionary,\n",
        "                            passes=10,\n",
        "                            minimum_probability=0.0,\n",
        "                            normalize=False,\n",
        "                            )\n",
        "        # Ранжирование тем\n",
        "        topics_popular = GetListPopularTopics(lda_model, bag_of_words)\n",
        "        # topics_prim = GetListPrimTopics(lda_model, topics_popular)\n",
        "\n",
        "        # Формирование аномальных тем\n",
        "        anomal_topics_content = topics_popular[-3:]\n",
        "        anomal_topics_context = GetTopInContext(topics_popular,\n",
        "                                                lda_model,\n",
        "                                                dictionary,\n",
        "                                                50)[-3:]\n",
        "\n",
        "        # y_predict_content = GetPredict(lda_model, bag_of_words,\n",
        "        #                         anomal_topics_content)\n",
        "        y_predict_context = GetPredict(lda_model, bag_of_words,\n",
        "                                anomal_topics_context)\n",
        "\n",
        "        # y_predict_content /= y_predict_content.max()\n",
        "        # y_predict_context /= y_predict_context.max()\n",
        "\n",
        "        # auc_content = roc_auc_score(y, y_predict_content)\n",
        "        auc_context = roc_auc_score(y, y_predict_context)\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        print(\"Эксперимент №{}/{}  с normal = {}, anomal = {}\".format(\n",
        "            experimant_cnt, all_experiments, c1, c2))\n",
        "        # print(\"Контент: roc_auc_score = {}\".format(auc_content))\n",
        "        print(\"Контекст: roc_auc_score = {}\".format(auc_context))\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # auc_list_content.append(auc_content)\n",
        "        auc_list_context.append(auc_context)\n",
        "\n",
        "# auc_np_content = np.array(auc_list_content)\n",
        "auc_np_context = np.array(auc_list_context)\n",
        "print(\"*\" * 50)\n",
        "# print(\"Медиана auc_content = {}\".format(np.median(auc_np_content)))\n",
        "# print(\"Среднее auc_content = {}\".format(np.mean(auc_np_content)))\n",
        "print(\"NMF Медиана auc_context = {}\".format(np.median(auc_np_context)))\n",
        "print(\"NMF Среднее auc_context = {}\".format(np.mean(auc_np_context)))\n",
        "print(\"*\" * 50)\n",
        "print(\"NMF Эксперименты завершены!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "4XhUCVEF1eYC",
        "outputId": "3543d324-8fea-4b3e-c2fc-41134d333296"
      },
      "source": [
        "# for NMF\r\n",
        "# !pip install \"gensim==3.8.1\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/93/c6011037f24e3106d13f3be55297bf84ece2bf15b278cc4776339dc52db5/gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.4.1)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuTyCV5L-CNw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}